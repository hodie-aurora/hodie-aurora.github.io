---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate Loki 高基数标签问题            		# 标题 
subtitle:   Kubemate Loki 高基数标签问题				#副标题
date:       2025-06-18				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
### 复杂问题一：高基数标签导致的Loki摄入延迟与内存泄漏

loki会将标准日志输出的每一个字段作为一个标签便于查询，这些字段最好是有固定值的字段，比如月份、组织编号等等数量比较少的字段，如果使用userid这种字段作为一级日志输出就会导致标签基数不断扩增，引发查询故障

**1. 问题表现**

我们在为齐鲁银行测试环境上云的时候，在平台和应用都部署完成之后在压力测试的时候，发现我们部署的信贷应用日志延迟非常高，大概应用输出标准日志之后大概5、6分钟才会被loki采集到，并且日志查询变的非常慢，而且 Loki Ingester 的 pod 频繁触发内存告警。

Loki的Ingester组件负责接收日志并将其写入内存中的流（stream），然后定期刷写到后端存储。如果写入路径（包括内存中的索引流管理）出现瓶颈，会导致日志堆积，进而影响查询性能。
我当时的猜想是，loki写入路径可能到达瓶颈，达到了影响读取性能的程度

**2. 排查过程**

排查步骤如下：

1. 首先我们检查网络延迟、丢包率，并且loki部分查询是正常的，只有信贷业务相关的标签查询不正常，于是我认为可能由于信贷后端故障导致的。
2. 我们重点监控了Loki的 `loki_ingester_memory_streams`（内存中的日志流数量）指标。发现这个指标在部分Ingester实例上呈持续增长，这是典型的内存泄漏模式，因为不符合Ingester内存周期性刷写的正常行为。
3. 基于指标表现，我猜测可能是由于信贷应用存在高基数标签导致了loki索引膨胀
4. 为验证该假设，我们直接调用了Loki的元数据API `GET /loki/api/v1/label/{label_name}/values`。通过脚本分析发现，一个名为 `trace_id`的标签，其唯一值的数量在短时间内增长至百万级别，从而确认了高基数标签是问题的根源。

**3. 问题根源分析**

* 信贷应用开发的时候把 `trace_id `作为日志的顶级字段打印出来了，我们部署的Promtail配置会自动将日志的顶级字段为Loki标签，而trace_id对应的值非常的多，基数非常高，而loki会为每个唯一的标签组合在内存中维护一个独立的索引流，当 `trace_id`这个高基数字段成为标签后，导致Loki Ingester需要维护数百万个索引流，迅速耗尽了分配给Pod的内存。内存压力使得Ingester无法高效地将日志块刷写到后端的MinIO对象存储，最终引发了我们观察到的摄入延迟和内存告警。

**4. 解决方案**

我们实施了分阶段的解决方案：

1. **立即缓解** ：通过修改Promtail的 `pipeline_stages`配置，增加一条 `labeldrop`规则，在日志采集端就将 `trace_id`标签丢弃。此操作阻止了问题的进一步恶化。随后，对Loki Ingester的StatefulSet执行了一次滚动重启，以释放已占用的内存，使服务恢复正常。
2. **系统性优化** ：为从根本上杜绝此类问题，我们在Kubemate平台中开发了一个**“高基数标签自动巡检”**功能。

* **实现方式** ：在我们的Go后端服务中增加一个定时任务，该任务定期调用Loki的API来分析所有标签的基数。
* **核心能力** ：当系统检测到某个标签的基数在短时间内异常增长并超过预设阈值时，会立即向平台管理员发送告警，并可配置触发 **自动熔断机制** ——即动态地将该标签加入全局的 `labeldrop`黑名单中。

1. **流程改进** ：我们将“禁止使用高唯一性ID作为日志标签”这一原则更新到了平台的最佳实践文档中，并与客户的开发团队进行了技术交流，推动其纳入内部的开发规范。
