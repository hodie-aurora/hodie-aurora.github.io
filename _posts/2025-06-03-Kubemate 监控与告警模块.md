---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate 监控与告警模块            		# 标题 
subtitle:   Kubemate 监控与告警模块				#副标题
date:       2025-06-18				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
### 监控与告警模块

该模块之所以复杂，核心在于它并非使用单一工具，而是 **通过组合多个分布式组件，解决 Prometheus 在生产环境下的高可用和数据持久化两大难题** 。

#### 为什么及如何复杂

一个单独的 Prometheus 简单易用，但它有天生缺陷：

1. **单点故障** ：如果 Prometheus 宕机，整个监控系统就瘫痪了。
2. **数据易失** ：监控数据默认存在本地磁盘，无法长期保存，也难以扩展。

为了解决这两个问题，项目引入了  **Thanos** ，这正是复杂性的来源。它将一个简单的单体监控，变成了一套分布式的、需要多个组件协同工作的系统。

#### 复杂在哪里

复杂性体现在以下几个关键点：

1. **组件协同** ：不再是管理一个 Prometheus，而是要确保 Prometheus、Thanos Sidecar、Thanos Querier、Thanos Store Gateway 和 MinIO 等多个组件之间的网络通信、配置和版本都能正确协作。
2. **高可用架构** ：为了避免单点故障，需要运行至少两套 Prometheus 实例。而 Thanos Querier 必须能正确地从这两套实例中查询数据，并进行 **去重** ，保证用户看到的是一份准确的数据。这个去重逻辑和配置就是一大复杂点。
3. **复杂的查询路径** ：用户的查询请求（例如来自仪表盘）不再是直接发给 Prometheus。它会发送给 `Thanos Querier`，后者需要智能地决定：

* 去哪里查询**最近2小时**的实时数据（找 Prometheus 的 Sidecar）。
* 去哪里查询**更早**的历史数据（找访问 MinIO 的 Store Gateway）。
* 最后将两部分数据合并，返回给用户。这条链路的维护和故障排查很复杂。

#### 具体的架构

这个模块的架构可以简化为以下几个层次：

* **采集层** ：部署多个**高可用**的 `Prometheus` 实例，它们各自独立地从 Kubernetes 集群中拉取监控指标。
* **连接与持久化层** ：
* 每个 `Prometheus` 旁都部署一个 `Thanos Sidecar` 组件。它的作用有两个：一是让查询层能访问到这个 Prometheus 的实时数据；二是定期将 Prometheus 的历史数据块上传到 `MinIO` 对象存储中。
* `MinIO` 作为统一的、廉价的 **长期存储后端** 。
* **查询层** ：
* `Thanos Querier` 是全局的 **统一查询入口** 。所有仪表盘和查询都通过它进行。它会连接到所有的 `Thanos Sidecar` 和 `Thanos Store Gateway`，提供全局数据视图。
* `Thanos Store Gateway` 部署在 `MinIO` 前面，它能让 `Querier` 查询存储在 MinIO 中的历史数据。
* **告警层** ：每个 `Prometheus` 独立计算告警规则，并将触发的告警发送给统一的 `Alertmanager`。`Alertmanager` 负责对来自多个 Prometheus 的告警进行去重、分组，然后通过邮件等渠道发送出去。

简单来说，其复杂度在于 **用一套分布式架构（Thanos）将多个独立的 Prometheus 实例整合成一个逻辑上统一、具备高可用性和无限历史数据存储能力的监控平台** 。
