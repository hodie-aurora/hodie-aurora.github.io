---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate QA01            		# 标题 
subtitle:   Kubemate QA01				#副标题
date:       2025-06-03				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
**模块1：自动化 Kubernetes 管理**

* **普通问题 (20)**
  1. Kubemate 是如何通过 Go 和 Gin 框架封装 Kubernetes API 的？能否举例说明一个简化操作的场景？ 
     就拿deploy来举例吧，比如你要查询某个命名空间的deploy内容，就要调用对应的deploy查询接口，我们假设这个接口叫deploylist，前端会发送你的查询请求，带上你的权限信息、以及你查询的参数，比如命名空间，以RESTful形式传递给后端接口之后会进行鉴权，使用go的k8sapi像apiserver发起请求，查看该用户有哪些命名空间的权限，将返回的结果与作为参数的命名空间进行对比，如果有该权限，则对apiserver发起查询deploy请求，没有则返回没有改命名空间查询权限以及对应的错误码，用户查询deploy详情也是一样的流程，接下来假如用户想调整某个deployA的副本数，用户可以直接点击调整副本数量按钮到理想的副本数量，点击确认按钮，发送调整副本数请求，此时后端接受到请求，进行鉴权，权限通过使用go k8spai向apiserver发送deployA的副本数变化请求，拿到请求返回的内容，比如调整成功，返回给前端刷新调整结果。
  2. 平台支持哪些常见的 Kubernetes 资源操作？（如 Pod, Deployment, Service, Ingress 等）
     大多数的常见k8s资源操作都支持，比如deploy、pod、statefulset、daemonset、service、job、cornjob、configmap、sercert、pvc、pv、存储类、，至于网关我们之前是使用treafik的ingressroute，目前换成了gateway的httproute还在推广，至于像CRD、CR等操作也支持，HPA等操作也会支持。
  3. RBAC 是如何配置和应用的？支持哪些粒度的权限控制？
     我们平台的权限的这样管理的，我们的权限体系是在k8s的权限体系上封装了一层，并使用CRD存储，举个例子：
     假如你使用我们的kubemate平台创建了一个用户，那么此时会创建一个对应用户CRD的CR来存储你的账号、密码信息，并且同步创建一个serviceAccount，把这个serviceaccount的信息也存储到CR里，然后我们会有一些角色，也是使用CRD进行存储的，比如你创建一个角色，会创建角色CRD对应的CR的同时，也会创建对应的role、或clusterrole，这取决于这个角色是否跨命名空间，如果跨命名空间则创建clusterrole以及对应的命名空间权限，如果是单纯的role就记录其对应的命名空间权限，此时如果你想使用用户绑定某个角色，用户的CR里记录绑定并且创建对应的rolebindding或clusterrolebinding，我们在k8s权限系统上封装一层的好处是，不仅避免了处理role或clusterrole的混乱，还更方便我们拓展权限
     假如此时用户A访问某个接口，我们可以根据用户A的用户的CR来获取其角色的CR，获取其CR对应role和clusterrole对应的命名空间，再根据传递的参数，我们就清楚其是否具备这个命名空间的权限了，其实后端做权限校验是为了更保险，我们在页面显示的时候就会根据其对应的权限来显示其仅有权限的命名空间等资源，我们对权限的拓展是无法使用k8s权限体系来控制的部分，比如某个权限比较低的用户，我并不想让其使用CICD流程，那么我就可以在CR里存储菜单的编码，不展示对应CICD的菜单，并且在后端进行鉴权，避免其越狱
  4. Namespace 如何用于实现多租户隔离？租户间的网络策略是如何配置的？
     namespace实现多租户隔离的权限部分如我刚才所说，我们会给每个命名空间创建单独的网关，是使用traefik加上gateway来实现的，treafik3.0之后允许创建多个网关，我们在每个命名空间再创建一个gateway，每个命名空间的路由通过httproute来控制。每个命名空间在创建的时候会为其自动创建traefik的Entrypoint，并创建命名空间对应的新的GatewayClass和gateway来监听这个Entrypoint，用户使用页面创建Httproute即可进行路由配置。
  5. Vue3 和 PrimeVue 构建的控制台提供了哪些核心的集群管理功能？
     首先就是k8s集群的监控，以及应用的发布部署，和应用对应的各种操作，比如说给应用暴漏服务，修改配置，挂载存储卷，设置路由，版本回滚，进入容器（❌），查看容器事件、日志，调节副本数，设置自动扩容，设置灰度发布，查看应用监控，链路追踪，应用拓扑，日志采集，CICD，云环境巡检等等很多内容
  6. 用户如何通过控制台创建和管理一个应用？
     这个很简单，比如使用deployment来举例，你可以点击创建按钮，选择该应用的命名空间，确认该应用的名称，并且可以给这个应用些一些注释，然后再点击下一步，选择应用镜像，这个会从已经配置好的镜像仓库配置文件中获取有哪些镜像，你可以选择对应的镜像，并且确定其暴漏的端口号，以及CPU和内存限制，并且可以继续设置其三种探针（❌），设置环境变量，设置启动命令，设置副本数量，设置滚动更新最大不可用副本数量，设置容器荚调度规则等等，然后下一步可以选择其需要挂载的configmap或secret并且输入对应的挂载路径，也可以为其挂载存储请求，或是空挂载卷或者本地挂载卷，最后可以选择这个deployment调度到哪个节点上，当然也可以不选择并交给k8s去根据k8s的调度算法部署，还可以给其打标签。在deployment创建成功之后，会自动提示要不要创建对应的服务，如果你创建对应的服务会自动绑定刚才的deployment，并且由你选择暴漏方式，是nodeport还是clusterIP，并且选择其对应的暴漏接口进行暴漏，最后你可以为其配置ingressroute或是httproute为其选择路由进行暴漏。
  7. 平台是否支持 Kubernetes 版本的升级管理？
     暂不支持，金融客户非常注重稳定性，对于k8s版本升级的意愿非常低，甚至可以说是比较抵触
  8. 对于集群节点的管理，平台提供了哪些功能（如添加、删除、状态监控）？
     对集群节点的管理，节点的添加和删除需要手动进行，我们的平台仅仅是能识别到集群内的全部节点，毕竟每台设备的系统、依赖可能存在差异，对于我们的客户来说这又是一个很冷门的需求，所以添加和删除节点仍需手动执行命令，至于状态监控的话，我们会监控节点的各种指标，比如说CPU使用占比、内存使用占比、磁盘使用占比、以及CPU核数、内存容量、磁盘容量、已经分配出去的CPU资源、分配出去的内存资源、pod的数量，事件的通知等等
  9.  Kubemate 如何处理 Kubernetes 的事件（Events）并呈现给用户？
      这个事件展示主要分为几部分，第一部分就是在主界面展示的集群有问题的事件信息，毕竟如果能通过仪表盘直观的看到目前集群有什么有问题的事件，这个比较容易引起操作者注意，及早发现和处理问题
      第二部分就是像deployment、statefulset、daemonset、pod这种资源，它的详情页面是一个tab页，有一个单独的tab页可以看到目前该资源全部的事件
      第三部分就是alertmanager的事件通知，你可以在告警规则界面配置告警规则，基于LogQL从Loki查询k8s事件（reason=FailedScheduling）触发告警条件，比如pod调度失败超过五次，通过多渠道推送通知
  10. 是否支持 Helm Chart 的部署和管理？ 
      目前不支持helm chart部署和管理，kubemate平台本身就是为了屏蔽直接操作k8s集群的复杂性设计的，如果使用helm来部署和管理与我们平台的概念相违背
  11. 平台如何管理 ConfigMap 和 Secret？
      平台管理configmap和secret的方式和管理其他资源大体相同，也是通过命名空间进行查询过滤，我们提供了可以对configmap和secret进行各种操作的页面，提供新增、修改、删除、查询、回滚等等操作，具体的操作就是，比如你点击编辑configmap，回弹出一个窗口，configmap的详情信息，因为一个configmap里可以有多个配置文件，所以是以list的方式来编辑configmap的详情，list分为左右两侧，左边是配置文件名称，比如application.yaml，右侧是具体的配置内容，可以点击右侧输入框，会弹出一个更大的窗口方便编辑，也可以对配置文件的value进行加密，使用的是我们自研的加密算法，避免数据库等配置泄漏，我们部署的应用都为我们自己开发的应用，会包含解密算法，保证配置安全
  12. 对于 PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) 的管理，平台提供了哪些支持？ 
      我们提供了PVC和PV的新增、编辑、删除、查询等等支持，并且不止PVC。我们对StorageClass也提供了增删改查等各种操作，比如PVC可以选择存储类型、访问模式、容量、存储卷等，存储卷可以选择自动挂载，也可以手动挂载已经创建好的存储卷。
      至于存储卷，创建的时候可以选择存储类型、访问模式、容量、回收策略、实现方式（比如nfs或是本地）等等
  13. 用户能否自定义 Kubernetes 资源的 YAML 文件进行部署？
      当然可以，不过使用yaml资源直接进行部署可能会存在风险，这项功能只会对管理员进行开放，与之类似的是导出整个命名空间全部资源的yaml，以及导入整个yaml压缩包内众多直接进行部署，这在集群迁移是时候非常方便，具体的实现方式就是，先对上传的压缩包进行解压，并且逐个执行对于的yaml，并返回执行结果，说明哪些资源执行成功，哪些资源执行失败
  14. 平台如何展示集群的整体资源使用情况（CPU, 内存, 存储）？
      平台有展示整体资源的使用情况分为几处，第一处就是仪表盘，可以展示整个集群的资源使用情况，包括集群内所有的cpu总数及其使用率、内存总数及其使用率、磁盘总数及其使用率、pod总数以及告警和图标等等
      然后还有一个节点页面，专门展示集群内各个节点的使用情况，其实这个节点使用情况在仪表盘也会展示，只不过比较简略，节点详情内可以看到节点Prometheus的采集数据，可以看到各个时间段的资源占比，以提供优化手段或报错排查
      其实每个pod都有这个功能，不过既然讨论的是集群资源使用情况，这点就先忽略不计了
  15. 是否支持集群联邦（Cluster Federation）或多集群管理功能？
      目前不支持集群联邦或多集群管理功能，我们通常会搞同城双活（❌），但是我们不提供直接管理两个集群，我们会在每个集群单独安装一份kubemate系统。
  16. 在简化 Kubernetes API 方面，有没有考虑过使用 Operator SDK 或 Kubebuilder 来创建 CRD？
      我为milvus operator做过开源贡献，我个人认为我们的kubemate平台没有必要单独搞operator，因为就一个无状态发布，直接由k8s监管即可，过于简单没有必要使用operator，协调循环交给k8s即可
  17. 平台的 API 是否有版本控制和向后兼容策略？
      对的，我们的api是向后兼容的，比如说我们目前升级了traefik的ingressroute到gateway的httproute，但是ingressroute仍可以正常使用，我们尽量通过推广和更便捷的操作来改变操作者的使用习惯。我们的API是向后兼容的
  18. 如何确保用户通过 Kubemate 进行的操作符合最佳实践，避免错误配置？
      首先我们是模板话操作，错误配置的可能很小，并且我们会使用权限校验，避免越权，云环境巡检模块会定期巡检，配合altermanager的规则及时发现错误并推送，并且我们会记录每步操作，提供日后排查。就是记录每步日志以及操作人员、时间等信息，被loki采集并定期备份到对象存储
  19. 对于 Kubernetes 中废弃 (deprecated) 的 API 版本，Kubemate 是如何处理和迁移的？
      我们采取了几个长期稳定版，金融客户对稳定版升级的意愿不大，要升级依赖的k8s底座同步升级
  20. 控制台的响应速度和用户体验如何？在高负载下表现如何？
      我们的平台主要是为运维人员开发的，基本上不存在高负载场景，如果针对应用的高负载，会根据HPA进行自动扩缩容，并且可以配合traefik的中间件进行限流、熔断等等操作

* **刁钻问题 (5)**
  1. 当底层 Kubernetes API 版本发生重大变更（例如某些 API 被移除或替换）时，Kubemate 如何保证平滑过渡，并最大限度减少对用户的影响？
  我们采取了几个长期稳定版，金融客户对稳定版升级的意愿不大，要升级依赖的k8s底座同步升级
  2. 在多租户场景下，如果一个租户的错误操作（如创建大量无用资源）可能影响到 Kubernetes API Server 的性能，Kubemate 有哪些机制来限制或缓解这种情况？ 
  为了防止单租户错误操作，kubemate有多层防御机制，首先就是命名空间实现租户硬隔离，通过权限系统精确控制其权限，并且为每个命名空间分配的资源总量设置上限，可以从根本上解决单租户误操作导致资源耗尽的情况，还有就是limitRange限制单个容器的资源消耗，配合Prometheus和altermanager进行监控告警，要是发生意外状况第一时间通知SRE处理，并且记录操作日志方便问题排查和经验总结，确保平台稳定性
  3. Kubemate 封装的 API 与直接使用 kubectl 或 Kubernetes 客户端库相比，在灵活性和功能完整性上是否存在一些妥协？如何平衡易用性和功能的全面性？
  当然，任何封装都会造成灵活性的损失，我们对于普通用户提供UI进行操作k8s集群，对于管理员用户仍能通过monakco编辑器直接修改各种资源的yaml，从而提供灵活性的同时保证权限安全。并且kubemate平台没有禁止用户使用kubectl，在拥有服务器权限的情况下，用户仍能通过kubectl对服务器进行管理
  4. 对于 CRD (Custom Resource Definitions) 的管理，Kubemate 提供了多大程度的支持？用户能否通过 Kubemate 的界面或 API 来管理自定义资源？
  当然，我们支持CRD与CR的管理，包括CRD与CR的查看、新增、修改、删除，我们的权限系统就建立在CRD的机制之上，但是CRD操作功能仅对管理员开发，用户可以通过页面查看、删除、回滚、新增、编辑yaml资源，但是编辑的话没有特定的可视化页面，只能通过monako编辑器对yaml进行编辑修改
  5. 如果 Kubemate 本身的管理组件（运行在 Kubernetes 之上）发生故障，用户将如何访问和管理其 Kubernetes 集群？是否有应急预案？
  如果kubemate发生故障，管理员仍然可以直接通过kubectl去管理集群，并不会对集群本身和部署的应用造成影响，kubemate可以使用多副本部署来保证高可用，如果发生严重故障时可以使用自动化脚本快速重新部署平台，实现灾难快速恢复


