---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate QA01            		# 标题 
subtitle:   Kubemate QA01				#副标题
date:       2025-06-03				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
**模块1：自动化 Kubernetes 管理**

* **普通问题 (20)**
  1. Kubemate 是如何通过 Go 和 Gin 框架封装 Kubernetes API 的？能否举例说明一个简化操作的场景？ 
     就拿deploy来举例吧，比如你要查询某个命名空间的deploy内容，就要调用对应的deploy查询接口，我们假设这个接口叫deploylist，前端会发送你的查询请求，带上你的权限信息、以及你查询的参数，比如命名空间，以RESTful形式传递给后端接口之后会进行鉴权，使用go的k8sapi像apiserver发起请求，查看该用户有哪些命名空间的权限，将返回的结果与作为参数的命名空间进行对比，如果有该权限，则对apiserver发起查询deploy请求，没有则返回没有改命名空间查询权限以及对应的错误码，用户查询deploy详情也是一样的流程，接下来假如用户想调整某个deployA的副本数，用户可以直接点击调整副本数量按钮到理想的副本数量，点击确认按钮，发送调整副本数请求，此时后端接受到请求，进行鉴权，权限通过使用go k8spai向apiserver发送deployA的副本数变化请求，拿到请求返回的内容，比如调整成功，返回给前端刷新调整结果。
  2. 平台支持哪些常见的 Kubernetes 资源操作？（如 Pod, Deployment, Service, Ingress 等）
     大多数的常见k8s资源操作都支持，比如deploy、pod、statefulset、daemonset、service、job、cornjob、configmap、sercert、pvc、pv、存储类、，至于网关我们之前是使用treafik的ingressroute，目前换成了gateway的httproute还在推广，至于像CRD、CR等操作也支持，HPA等操作也会支持。
  3. RBAC 是如何配置和应用的？支持哪些粒度的权限控制？
     我们平台的权限的这样管理的，我们的权限体系是在k8s的权限体系上封装了一层，并使用CRD存储，举个例子：
     假如你使用我们的kubemate平台创建了一个用户，那么此时会创建一个对应用户CRD的CR来存储你的账号、密码信息，并且同步创建一个serviceAccount，把这个serviceaccount的信息也存储到CR里，然后我们会有一些角色，也是使用CRD进行存储的，比如你创建一个角色，会创建角色CRD对应的CR的同时，也会创建对应的role、或clusterrole，这取决于这个角色是否跨命名空间，如果跨命名空间则创建clusterrole以及对应的命名空间权限，如果是单纯的role就记录其对应的命名空间权限，此时如果你想使用用户绑定某个角色，用户的CR里记录绑定并且创建对应的rolebindding或clusterrolebinding，我们在k8s权限系统上封装一层的好处是，不仅避免了处理role或clusterrole的混乱，还更方便我们拓展权限
     假如此时用户A访问某个接口，我们可以根据用户A的用户的CR来获取其角色的CR，获取其CR对应role和clusterrole对应的命名空间，再根据传递的参数，我们就清楚其是否具备这个命名空间的权限了，其实后端做权限校验是为了更保险，我们在页面显示的时候就会根据其对应的权限来显示其仅有权限的命名空间等资源，我们对权限的拓展是无法使用k8s权限体系来控制的部分，比如某个权限比较低的用户，我并不想让其使用CICD流程，那么我就可以在CR里存储菜单的编码，不展示对应CICD的菜单，并且在后端进行鉴权，避免其越狱
  4. Namespace 如何用于实现多租户隔离？租户间的网络策略是如何配置的？
     namespace实现多租户隔离的权限部分如我刚才所说，我们会给每个命名空间创建单独的网关，是使用traefik加上gateway来实现的，treafik3.0之后允许创建多个网关，我们在每个命名空间再创建一个gateway，每个命名空间的路由通过httproute来控制。每个命名空间在创建的时候会为其自动创建traefik的Entrypoint，并创建命名空间对应的新的GatewayClass和gateway来监听这个Entrypoint，用户使用页面创建Httproute即可进行路由配置。
  5. Vue3 和 PrimeVue 构建的控制台提供了哪些核心的集群管理功能？
     首先就是k8s集群的监控，以及应用的发布部署，和应用对应的各种操作，比如说给应用暴漏服务，修改配置，挂载存储卷，设置路由，版本回滚，进入容器（❌），查看容器事件、日志，调节副本数，设置自动扩容，设置灰度发布，查看应用监控，链路追踪，应用拓扑，日志采集，CICD，云环境巡检等等很多内容
  6. 用户如何通过控制台创建和管理一个应用？
     这个很简单，比如使用deployment来举例，你可以点击创建按钮，选择该应用的命名空间，确认该应用的名称，并且可以给这个应用些一些注释，然后再点击下一步，选择应用镜像，这个会从已经配置好的镜像仓库配置文件中获取有哪些镜像，你可以选择对应的镜像，并且确定其暴漏的端口号，以及CPU和内存限制，并且可以继续设置其三种探针（❌），设置环境变量，设置启动命令，设置副本数量，设置滚动更新最大不可用副本数量，设置容器荚调度规则等等，然后下一步可以选择其需要挂载的configmap或secret并且输入对应的挂载路径，也可以为其挂载存储请求，或是空挂载卷或者本地挂载卷，最后可以选择这个deployment调度到哪个节点上，当然也可以不选择并交给k8s去根据k8s的调度算法部署，还可以给其打标签。在deployment创建成功之后，会自动提示要不要创建对应的服务，如果你创建对应的服务会自动绑定刚才的deployment，并且由你选择暴漏方式，是nodeport还是clusterIP，并且选择其对应的暴漏接口进行暴漏，最后你可以为其配置ingressroute或是httproute为其选择路由进行暴漏。
  7. 平台是否支持 Kubernetes 版本的升级管理？
     暂不支持，金融客户非常注重稳定性，对于k8s版本升级的意愿非常低，甚至可以说是比较抵触
  8. 对于集群节点的管理，平台提供了哪些功能（如添加、删除、状态监控）？
     对集群节点的管理，节点的添加和删除需要手动进行，我们的平台仅仅是能识别到集群内的全部节点，毕竟每台设备的系统、依赖可能存在差异，对于我们的客户来说这又是一个很冷门的需求，所以添加和删除节点仍需手动执行命令，至于状态监控的话，我们会监控节点的各种指标，比如说CPU使用占比、内存使用占比、磁盘使用占比、以及CPU核数、内存容量、磁盘容量、已经分配出去的CPU资源、分配出去的内存资源、pod的数量，事件的通知等等
  9.  Kubemate 如何处理 Kubernetes 的事件（Events）并呈现给用户？
      这个事件展示主要分为几部分，第一部分就是在主界面展示的集群有问题的事件信息，毕竟如果能通过仪表盘直观的看到目前集群有什么有问题的事件，这个比较容易引起操作者注意，及早发现和处理问题
      第二部分就是像deployment、statefulset、daemonset、pod这种资源，它的详情页面是一个tab页，有一个单独的tab页可以看到目前该资源全部的事件
      第三部分就是alertmanager的事件通知，你可以在告警规则界面配置告警规则，基于LogQL从Loki查询k8s事件（reason=FailedScheduling）触发告警条件，比如pod调度失败超过五次，通过多渠道推送通知
  10. 是否支持 Helm Chart 的部署和管理？ 
  11. 平台如何管理 ConfigMap 和 Secret？
  12. 对于 PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) 的管理，平台提供了哪些支持？ 
  13. 用户能否自定义 Kubernetes 资源的 YAML 文件进行部署？
  14. 平台如何展示集群的整体资源使用情况（CPU, 内存, 存储）？
  15. 是否支持集群联邦（Cluster Federation）或多集群管理功能？
  16. 在简化 Kubernetes API 方面，有没有考虑过使用 Operator SDK 或 Kubebuilder 来创建 CRD？
  17. 平台的 API 是否有版本控制和向后兼容策略？
  18. 如何确保用户通过 Kubemate 进行的操作符合最佳实践，避免错误配置？
  19. 对于 Kubernetes 中废弃 (deprecated) 的 API 版本，Kubemate 是如何处理和迁移的？
  20. 控制台的响应速度和用户体验如何？在高负载下表现如何？
* **刁钻问题 (5)**
  1. 当底层 Kubernetes API 版本发生重大变更（例如某些 API 被移除或替换）时，Kubemate 如何保证平滑过渡，并最大限度减少对用户的影响？
  2. 在多租户场景下，如果一个租户的错误操作（如创建大量无用资源）可能影响到 Kubernetes API Server 的性能，Kubemate 有哪些机制来限制或缓解这种情况？ 
  3. Kubemate 封装的 API 与直接使用 kubectl 或 Kubernetes 客户端库相比，在灵活性和功能完整性上是否存在一些妥协？如何平衡易用性和功能的全面性？
  4. 对于 CRD (Custom Resource Definitions) 的管理，Kubemate 提供了多大程度的支持？用户能否通过 Kubemate 的界面或 API 来管理自定义资源？
  5. 如果 Kubemate 本身的管理组件（运行在 Kubernetes 之上）发生故障，用户将如何访问和管理其 Kubernetes 集群？是否有应急预案？
