---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate QA02            		# 标题 
subtitle:   Kubemate QA02				#副标题
date:       2025-06-03				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
**模块2：监控与告警**

**普通问题 (20)**

第一部分：Prometheus 和 Thanos

1. Prometheus 是如何部署的？采用了哪些高可用配置？
   我们的 Prometheus 采用双副本模式部署以实现高可用。每个 Prometheus 实例都配置了唯一的外部标签，通过标签将 Thanos 进行数据去重。这种架构确保了单个 Prometheus 实例故障不会导致监控数据采集中断。
   为了实现长期存储和全局查询，我们集成了 Thanos。每个 Prometheus Pod 内都运行一个 Thanos Sidecar 容器，它负责两件事：一是将 Prometheus 本地生成的 TSDB 数据块上传到 MinIO 对象存储；二是通过 gRPC 接口向 Thanos Querier 提供该实例内存中的实时数据。
   Thanos Querier 作为统一查询入口，会聚合所有 Sidecar 的实时数据和 Store Gateway 提供的历史数据。它根据外部标签对来自不同副本的数据进行合并去重，从而提供一个统一的全局数据视图。Thanos Store Gateway 组件的作用是连接 MinIO，使其存放的海量历史数据可以被查询。最后，Thanos Compactor 作为一个后台服务，负责对 MinIO 中的数据块进行压缩、降采样和管理数据生命周期，以此来优化存储成本和历史数据查询性能。
2. Thanos 的哪些组件被部署了（如 Sidecar, Querier, Store Gateway, Compactor）？它们各自的作用是什么？具体是如何部署的（不要提供代码）？
   我们部署了 Thanos 的四个核心组件：Sidecar、Querier、Store Gateway 和 Compactor。
   Thanos Sidecar 与 Prometheus 部署在同一个 Pod 中，作为一个伴生容器。这种部署方式使其可以直接访问 Prometheus 的数据卷。它的作用是上传历史数据块到 MinIO，并向 Querier 暴露一个 gRPC 接口以查询该 Prometheus 实例的实时数据。
   Thanos Querier 以无状态、可水平扩展的 Deployment 方式部署，并通过 Service 对外提供查询服务。它的作用是接收 PromQL 查询，然后将请求分发给所有的 Sidecar 和 Store Gateway，最后将返回的结果进行聚合与去重，形成最终的全局数据。
   Thanos Store Gateway 同样以无状态的 Deployment 部署，可以按需扩缩容。它的职责是扫描 MinIO 中的数据块元数据，并向 Querier 提供一个 gRPC 接口，使得对象存储中的历史数据能够被查询。
   Thanos Compactor 作为一个单例组件部署，通常是一个单副本的 Deployment。它的作用是在后台对 MinIO 中的数据进行优化，包括将小数据块合并成大块、对老数据进行降采样以提升长期查询效率，以及执行数据保留策略。
3. MinIO 是如何作为 Thanos 的长期存储的？存储桶的配置和生命周期管理是怎样的？
   minio使用thanos持久化Prometheus数据的工作流程大概是：
   连接配置：
   1：需要与minio交互的thanos组件都通过挂载configmap配置文件存储minio endpoint、访问密钥、私有密钥、存储桶等信息与minio进行通信
   2：数据上传与访问：Thanos Sidecar 会定期将 Prometheus 本地生成的 TSDB 数据块上传到这个指定的 MinIO 存储桶中。而 Thanos Store Gateway 则会监控这个存储桶，将其中数据块的元数据索引在内存中，以便 Thanos Querier 能够查询到这些历史数据。
   关于存储桶的配置和生命周期管理：
   存储桶配置：我们的thanos有专用的存储桶并且关闭了版本控制，因为thanos数据块是不可变的，关闭版本控制可以避免不必要的开销，这个存储桶仅允许thanos所在的命名空间进行读写操作
   生命周期管理：我们没有使用minio自身的生命周期管理策略来管理监控数据，我们通过Thanos Compactor 来执行数据保留，我们的保留策略是原始精度数据 14 天，5 分钟降采样数据 90 天，1 小时降采样数据 1 年，Compactor 根据这些规则自动删除minio中过期的符合条件的数据块
   这种方式将数据管理的逻辑保留在 Thanos 层，与底层存储解耦，逻辑更清晰，也更容易适配不同的对象存储后端。
4. Prometheus 的抓取配置 (scrape configs) 是如何管理的？是否支持动态更新？
   我们的抓取配置规则是通过Prometheus operator的Service Monitor和Pod Monitor对应的CRD来实现的，我们平台会在每个命名空间创建的时候同步创建Service Monitor和Pod Monitor对应的CR，并且在像deploy、statefulset、daemonset、service这类资源被创建的时候同步打上命名空间对应的标签，即可被Prometheus operator自动采集，这样做可以避免不同租户之间互相干扰，也可以避免错误数据影响范围扩大。
5. 监控数据的查询性能如何？特别是在查询大范围时间序列数据时，Thanos 的降采样（down-sampling）是如何帮助提升性能的？
   短期查询和长期查询性能是不同的：
   短期查询（例如，过去几小时）：性能非常好。这类查询主要由 Thanos Querier 直接转发给 Thanos Sidecar，数据直接从 Prometheus 实例的内存中获取，响应速度非常快，通常在毫秒到秒级。
   长期查询（例如，过去几个月）：性能稍微慢一些，但是不是特别慢，大概在秒级别，3到5秒左右，因为 thanos compactor 会定期扫描原始数据块，在数据块达到规定的阈值时触发降采样，将原始数据计算出精度更低的版本进行存储，我们的规则的原始数据14天，5分钟降采样90天，1小时降采样一年。并且thanos querier受到查询时会分析查询的范围和步长，如果用户查询的是过去 30 天的数据，Querier 会自动选择使用 5 分钟或 1 小时分辨率的数据来响应，而不是去读取海量的原始数据。这样长期查询的性能可以提升到秒级
6. 什么是Prometheus指标的基数？你们是如何管理 Prometheus 指标的基数（Cardinality）问题的？有没有遇到过高基数带来的性能问题，又是如何解决的？
   我来解释一下这个问题，假如你有两个接口/metrics/AAA和/metrics/BBB，Prometheus采集之后就是两个时间序列，就是两个基数，假如/metrics/AAA这个接口有动态参数/metrics/AAA？userid=123456,这就会导致/metrics/AAA这个接口有多少动态参数请求就有多少时间序列，从而有多少基数，基数越大查询效率越低从而导致性能问题。我们也遇见过应用高基数带来的性能问题，这几乎是不可避免的，至于解决方案主要有以下几点：1、首先就是与开发团队沟通，尽量避免使用动态参数作为url，从而从根源上避免超高基数的产生。2、第二点就是聚合标签，如果我们不关心高基数的具体细节，只关心其聚合值。我们可以修改Service Montior从而修改配置记录规则（Recording Rules），预先计算好聚合后的指标，然后使用 labeldrop 丢弃原始的高基数指标，只保留聚合后的低基数指标。
7. Thanos Querier 是如何对来自两个 Prometheus 副本的数据进行去重的？这个机制的原理是什么？
   这个原理其实非常简单，就是为每个Prometheus副本分别配置了不同的prometheus_replica这样的外部标签，采集的时候就会采集到两个几乎一样的时间序列，区别就是它们两个prometheus_replica这个外部标签不同，假如一共叫做prometheus_replica=“A”另一个叫做prometheus_replica=“B”，查询聚合的时候， Thanos Querier 发起一个 PromQL 查询时会查询到标签不同的两组结果，Thanos Querier会使用去重功能，比如两个序列都作为副本序列，Thanos Querier会选择哈希值最大的副本数据，确保在多次查询中结果一直且为一条连续时间序列。从而实现去重。
   （通过 --query.replica-label 参数指定要去重的标签，这里就是 prometheus_replica）
8. 你们为 Prometheus 和 Thanos 的各个组件设置了怎样的资源请求（requests）和限制（limits）？在资源规划上有什么考量？
   Prometheus + Thanos Sidecar：
   CPU: requests 较高，limits 更高。Prometheus 的抓取和规则评估是 CPU 密集型的。
   Memory: requests 和 limits 都设置得很高，并且通常相等。这是因为 Prometheus 的内存使用量与指标基数和抓取的目标数量强相关。我们不希望它因为内存 limits 被 OOMKilled，这会导致 2 小时内的监控数据丢失。我们会根据实际的基数来设定一个充裕的内存值，比如 32Gi 或更高。Sidecar 本身内存消耗不大。
   Thanos Querier：
   CPU: requests 中等，limits 较高。查询的复杂度和并发度会消耗大量 CPU。
   Memory: requests 和 limits 都比较高。Querier 需要在内存中处理从 Sidecar 和 Store Gateway 返回的数据，复杂的聚合查询会消耗大量内存。它是一个无状态组件，可以水平扩展，所以我们会部署多个副本分担压力。
   Thanos Store Gateway：
   CPU: requests 较低，limits 中等。它的主要开销是响应 gRPC 请求，CPU 消耗通常不大。
   Memory: requests 和 limits 非常高。Store Gateway 启动时需要加载 MinIO 中所有数据块的元数据（索引和标签信息）到内存中。这个内存量与对象存储中数据块的总量成正比。这是我们监控的重点，如果内存持续增长，说明需要考虑优化或分片部署。
   Thanos Compactor：
   CPU & Memory: requests 中等，limits 较高。Compactor 在执行压缩和降采样任务时是资源密集型的，尤其是在处理大数据块时。但这些任务是周期性的，不是持续的。所以我们给它较高的 limits 以应对峰值使用，但 requests 可以相对保守。
   考量总结：
   稳定性优先：对于 Prometheus 和 Store Gateway 这种有状态或加载大量数据的组件，内存 requests 和 limits 设为相等，避免被驱逐或 OOMKilled。
   弹性伸缩：对于 Querier 这种无状态组件，我们会配置 HPA（Horizontal Pod Autoscaler），根据 CPU 和内存使用率自动扩缩容。
   成本与性能平衡：通过持续监控各组件的实际资源使用率，我们会定期调整 requests，以确保在 Kubernetes 集群中预留了足够的资源，同时又避免了过度浪费。

第二部分：Prometheus 和 Alertmanager

9. Alertmanager 支持哪些告警渠道？邮件和短信告警是如何配置的？告警的原理是什么？
   我们平台的Alertmanager支持短信告警和邮件告警，整个流程是 Prometheus -> Alertmanager -> 告警api

   * Prometheus 根据预设的告警规则（Alerting Rules）持续评估指标数据。
   * 当某个规则的条件被满足并持续超过 for 设定的时间后，Prometheus 会将这个告警状态标记为 firing，并把包含告警详情（标签、注解等）的数据推送给 Alertmanager。
   * Alertmanager 收到告警后，会先进行去重（deduplication）、分组（grouping）、抑制（silencing）和路由（routing）。
   * 根据路由规则，Alertmanager 决定将这个告警发送到哪个接收器（receiver），比如“邮件接收器”或“短信接收器”。
   * 接收器根据自身的配置，调用相应的服务（SMTP 服务器或短信 API）来发送最终的通知。
10. 平台预定义了哪些关键的告警规则？用户是否可以自定义告警规则？如果可以，是如何实现的？
    我们的内置规则集覆盖了从底层基础设施到上层应用的多个层面，主要包括：

   Kubernetes 集群层面：

* 节点状态异常：NodeNotReady、节点磁盘/内存/CPU 压力过大。
* 控制平面健康：APIServerDown、EtcdDown、Scheduler/ControllerManagerDown。
* 资源配额：ResourceQuotaReached，命名空间的资源即将耗尽。
* Pod 状态：大量 Pod CrashLoopBackOff、Pending 状态过久、Deployment 副本数不满足。

   监控系统自身 (Meta-Monitoring)：

* PrometheusDown、AlertmanagerDown。
* PrometheusTargetScrapeFailed：Prometheus 抓取某个目标失败。
* ThanosComponentDown：Thanos 的 Querier、Store、Compactor 等组件宕机。

   应用通用性能指标 (Golden Signals)：

* 高延迟（High Latency）：服务请求的 P95/P99 延迟超过阈值。
* 高错误率（High Error Rate）：HTTP 5xx 错误码比例过高。
* 低吞吐量（Low Throughput）：QPS 远低于正常水平。

   自定义告警规则的实现方式是通过Prometheus operator来实现的，我们可以通过kubemate UI直接查看、增加、修改、删除PrometheusRule这个CRD里CR的规则，用户通过填写告警名称 alert、评估表达式 expr、持续时间 for、以及描述告警信息的 labels 和 annotations。Prometheus Operator 会自动发现集群中所有 PrometheusRule 对象，动态地让 Prometheus 加载。从而实现自定义告警，比如“订单队列积压超过 1000 条”等等
11. 告警的抑制 (Silencing) 和去重 (Deduplication) 是如何通过 Alertmanager 实现的？请举例说明。
    去重 (Deduplication):
    去重是 Alertmanager 自动完成的，用户无需配置。它的机制是：Prometheus 会在每次评估周期（例如 1 分钟）都向 Alertmanager 发送处于 firing 状态的告警。如果同一个告警（具有完全相同的标签集）被多次发送，Alertmanager 只会处理第一个，后续重复的会被忽略。这样可以确保对于一个持续存在的问题，我们不会每分钟都收到一条通知
    抑制 (Silencing):
    抑制是需要手动创建的，我们的kubemate会集成altermanager的Silence，比如我要在明天凌晨2点到4点升级数据库，我就可以创建一个silence，创建一个匹配器 (Matchers)：就是一组标签键值对，用于匹配要抑制的告警。因为我要升级数据库，我可以创建一个匹配器 job="mysql"，severity="warning"，并标记时间范围，也就是silence的开始和结束时间，并且加上创建人和注释
    [假设我们计划在凌晨 2 点到 4 点对 prod-mysql-cluster 进行数据库升级。我们预知到在这段时间内，数据库的 HostDown 和 HighLatency 告警会被触发。为了避免不必要的告警轰炸，运维人员可以提前创建一个 Silence：
    Matchers: {cluster="prod-mysql-cluster", alertname=~"HostDown|HighLatency"} (匹配 prod-mysql-cluster 集群下，告警名称为 HostDown 或 HighLatency 的告警)
    时间: startsAt: 02:00, endsAt: 04:00
    备注: "Planned MySQL upgrade"
    当 Alertmanager 在这个时间窗口内收到符合上述匹配器的告警时，它会直接将这些告警标记为 inhibited（被抑制），而不会将它们发送出去。这样，运维团队就可以专注地进行维护工作，而不会被预料之中的告警所干扰。]
12.  告警通知中包含哪些关键信息，以帮助用户快速定位问题？模板是如何定制的？
     *  告警状态 (State)：是 [FIRING] 还是 [RESOLVED]，让用户一眼就知道问题是新发生的还是已解决。
     *  严重等级 (Severity)：例如 [Critical], [Warning], [Info]，帮助判断响应的优先级。
     *  告警名称 (Alertname)：简明扼要地概括问题，如 KubeNodeNotReady。
     *  摘要 (Summary/Description)：一句话描述问题，例如 “Node worker-3 has been unready for more than 5 minutes.”。
     *  关键标签 (Labels)：这是定位问题的核心。我们会包含 cluster, namespace, service, pod, node 等上下文信息，清晰地指出问题发生在哪里。
     *  触发时间 (Fired At)：问题开始发生的时间。
     *  仪表盘链接 (Dashboard Link)：一个直接指向相关监控仪表盘的 URL，用户点击后可以立即看到与告警相关的指标图表，极大地缩短了排障时间。
     *  预案链接 (Runbook Link)：对于一些常见或重要的告警，我们会附上一个指向内部知识库（Wiki）的链接，里面有处理该告警的标准操作流程（SOP）。
      Alertmanager 的通知模板是使用 Go 的 text/template 和 html/template 引擎来定制的。

    配置文件：在 alertmanager.yml 中，我们可以通过 template_files 指令指向一个或多个自定义的模板文件。
      模板文件：在模板文件中，我们可以访问 Alertmanager 传来的数据结构，这个结构中包含了告警的所有信息（Labels, Annotations, StartsAt, Status 等）。
      定制逻辑：我们可以使用模板语言的循环（range）、条件判断（if/else）和函数来构建我们想要的通知格式。例如，我们可以遍历一个告警组中的所有告警，将它们的摘要和标签格式化成一个清晰的列表。
      生成链接：对于仪表盘链接，我们通常是在 Prometheus 的告警规则的 annotations 中，利用 {{$labels.cluster }} 和 {{ $labels.pod }} 等变量动态生成 URL，然后 Alertmanager 模板直接引用这个 annotation 即可。
13.  请描述一个告警从 Prometheus 触发到通过 Alertmanager 发送到用户手中的完整生命周期。
      **阶段一：评估 (Pending)**
      触发点：Prometheus Server 根据其配置文件中的 alerting_rules，按照 evaluation_interval（例如，每分钟）的周期，持续对指标数据执行 PromQL 查询。
      状态：当某个告警规则的 expr 表达式首次返回了结果（即查询到非空的时间序列），这个告警就进入了 Pending 状态。此时，Prometheus 不会做任何事，只是在内部记录下这个状态。
      **阶段二：触发 (Firing)**
      触发点：如果同一个告警在 Pending 状态下，持续满足条件的时间超过了规则中定义的 for 持续时间（例如，for: 5m），Prometheus 就会将该告警的状态从 Pending 变为 Firing。for 子句的存在是为了防止因数据抖动而产生的瞬时告警。
      动作：一旦告警进入 Firing 状态，Prometheus 会立刻将这个告警的完整信息（包括所有标签和注解）打包，通过 HTTP 请求发送给在配置中指定的所有 Alertmanager 实例。
      **阶段三：处理 (Processing by Alertmanager)**
      接收：Alertmanager 实例接收到来自 Prometheus 的告警。
      抑制 (Silencing)：Alertmanager 首先检查该告警是否匹配任何当前生效的 Silence 规则。如果匹配，告警流程终止，不会发送通知。
      去重与分组 (Deduplication & Grouping)：如果不被抑制，Alertmanager 会根据 group_by 配置（例如，按 cluster, alertname 分组）将告警放入对应的分组中。同一分组的告警会在一个通知里发送，避免告警风暴。
      等待 (Wait & Aggreration)：Alertmanager 不会立即发送通知，而是会等待 group_wait 定义的一小段时间（例如 30 秒），看是否有更多相关的告警进入同一个分组。
      路由 (Routing)：等待结束后，Alertmanager 根据 route 配置树，决定这个告警组应该发送给哪个 receiver（例如，email-receiver 或 sms-receiver）。
      **阶段四：通知 (Notification)**
      发送：receiver 根据其配置（如 SMTP 服务器地址或 Webhook URL），使用定制的模板格式化告警信息，并将其发送出去。
      重复：只要告警在 Prometheus 端保持 Firing 状态，Alertmanager 会根据 repeat_interval（例如，每 4 小时）的设置，重新发送该告警通知，以提醒问题尚未解决。
      **阶段五：解决 (Resolved)**
      触发点：当 Prometheus 在下一次评估时，发现之前触发告警的 expr 表达式不再返回任何结果，说明问题已解决。
      通知：Prometheus 会立即向 Alertmanager 发送一条 Resolved 状态的通知。
      解决通知：Alertmanager 收到后，会根据配置（send_resolved: true）向相应的 receiver 发送一条解决通知，告知用户问题已经恢复，形成一个完整的闭环。
14.  你们如何处理告警风暴（Alert Storm）和告警抖动（Flapping）的问题？
      处理告警抖动 (Flapping):
      告警抖动指的是一个状态在正常和异常之间快速、反复地切换，导致告警不断地触发和解决。
      解决方案：我们主要使用 Prometheus 告警规则中的 for 子句来解决。
      原理：for: 5m 意味着一个告警条件必须持续满足 5 分钟，这个告警才会真正进入 Firing 状态。如果在这 5 分钟内，条件有任何一次不再满足，计时器就会重置。这就像一个“冷静期”，能有效地过滤掉那些由网络瞬时中断、服务重启等引起的短暂、无意义的告警，确保我们只关注持续存在的真实问题。

      处理告警风暴 (Alert Storm):
      告警风暴指的是当一个底层基础设施（如一个数据库、一个节点或整个机房）发生故障时，导致大量依赖于它的上层应用同时产生告警，瞬间淹没运维人员。
      解决方案：我们主要依赖 Alertmanager 的分组 (Grouping) 和抑制 (Inhibition) 功能。
      分组 (Grouping)：
      我们在 Alertmanager 的配置中定义了 group_by 规则，例如 group_by: ['cluster', 'alertname']。这意味着，如果一个集群中 100 个 Pod 同时因为节点故障而无法访问，我们不会收到 100 条通知，而是会收到一条聚合后的通知，标题可能是“[FIRING:100] KubePodNotReady in cluster-prod”，内容里列出了所有受影响的 Pod。这极大地减少了通知的数量。

      抑制 (Inhibition)：
      这是更高级的、基于依赖关系的处理方式。我们在 Alertmanager 中定义 inhibit_rules。
      举例：我们可以定义一条规则：如果 NodeDown 这个告警正在触发，那么就抑制（inhibit）所有来自该节点的 PodDown 或 ServiceUnavailable 告警。
      原理：这条规则的逻辑是，既然节点本身都宕机了，那么运行在它上面的 Pod 和 Service 不可用是意料之中的结果。我们只需要处理最根本的 NodeDown 告警即可，其他派生出来的告警会被自动静音。这帮助运维人员聚焦于问题的根源，而不是被大量的次生告警所淹没。
      通过 for、grouping 和 inhibition 这三个工具的组合使用，我们能够有效地将原始的、嘈杂的告警信号，提炼成少量、精准且可操作的通知。

15.  Alertmanager 的路由（routing）规则是如何配置的？能否举一个例子，比如将不同严重程度或不同业务线的告警发送给不同的人？
      Alertmanager 的路由规则通过配置文件定义，采用树状结构，从根路由开始，根据告警标签匹配，决定发送到哪个接收器（如邮件或短信）。
      配置方式：
      根路由指定默认接收器，所有告警按标签（如告警名称、集群）分组。
      子路由根据标签（如严重程度、业务线）匹配，指定特定接收器，支持嵌套和继续匹配（允许告警发送到多个接收器）。

第三部分：其他

16.  应用健康状态是如何定义的？通过哪些指标来衡量？
      延迟 (Latency)：
      定义：处理一个请求所需的时间。我们特别关注错误请求的延迟和成功请求的延迟。
      衡量指标：我们通常采集请求耗时的直方图（Histogram），例如 http_request_duration_seconds_bucket。通过这个指标，我们可以计算出 P99, P95, P50 等分位数延迟，并对它们设置告警阈值。高 P99 延迟通常是用户体验下降的第一个信号。
      Prometheus 检测：
      收集应用的 /metrics 端点暴露的 http_request_duration_seconds_bucket。
      使用 PromQL 查询，如计算 P99 延迟：histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))。
      设置告警，例如：P99 延迟超过 1 秒时触发通知（通过 Alertmanager 发送邮件和短信）。
      高 P99 延迟（如超过阈值）表示用户体验下降，需优化应用或扩容。
      
      流量 (Traffic)：
      定义：系统收到的总需求量。
      衡量指标：对于 HTTP 服务，这通常是每秒请求数（QPS），通过 rate(http_requests_total[5m]) 计算得出。对于其他系统，可能是每秒的数据库查询数或队列消息数。我们会监控流量的异常飙升或骤降。
      Prometheus 检测：
      收集 http_requests_total 指标，计算 QPS：rate(http_requests_total[5m])。
      对于数据库或队列，收集类似 db_queries_total 或 queue_messages_total，计算速率：rate(db_queries_total[5m])。
      设置告警，例如：QPS 骤升（如翻倍）或骤降（如接近 0）时触发通知。
      异常流量模式（如突增或突降）可能表示攻击、故障或配置错误。

      错误 (Errors)：
      定义：请求失败的速率。
      衡量指标：我们采集按状态码分类的请求总数，例如 http_requests_total{code="5xx"}。通过计算 rate(http_requests_total{code="5xx"}[5m]) / rate(http_requests_total[5m])，我们可以得到 5xx 错误率，并对其设置告警。任何非零的错误率都值得关注。
      Prometheus 检测：
      收集 http_requests_total 指标，按状态码过滤：rate(http_requests_total{code=~"5.."}[5m])。
      计算错误率：rate(http_requests_total{code=~"5.."}[5m]) / rate(http_requests_total[5m])。
      设置告警，例如：错误率超过 1% 时触发邮件和短信通知。
      非零错误率（如 5xx 增加）需立即检查，可能是代码 bug 或依赖故障。

      饱和度 (Saturation)：
      定义：服务的繁忙程度，衡量系统资源有多“满”，是衡量系统容量的关键。
      衡量指标：这通常是最难衡量的。
      对于强依赖内存的服务，我们会看内存使用率。
      对于 CPU 密集型服务，我们会看 CPU 利用率。
      对于数据库或队列系统，我们会看队列长度或副本延迟。
      饱和度指标的告警通常预示着系统即将达到瓶颈，能为我们争取到扩容或优化的时间。
      Prometheus 检测：
      内存：收集 container_memory_usage_bytes 或 node_memory_MemAvailable_bytes，计算使用率：100 * (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)。
      CPU：收集 container_cpu_usage_seconds_total，计算利用率：rate(container_cpu_usage_seconds_total[5m])。
      数据库/队列：收集 db_queue_length 或 replication_lag_seconds，设置阈值告警（如队列长度 > 1000 或延迟 > 10 秒）。
      设置告警，例如：CPU 使用率超过 80% 或队列长度过高时触发通知。
      高饱和度预示系统瓶颈，需扩容或优化资源。

      除了这四个信号，我们还会结合 Kubernetes 提供的健康状态，比如 Deployment 的可用副本数是否满足期望值 (kube_deployment_status_replicas_available)，以及 Pod 的重启次数 (kube_pod_container_status_restarts_total)。
      一个应用被认为是“健康”的，当且仅当它的延迟、错误率都在可接受的范围内，流量符合预期模式，饱和度没有达到危险水位，并且其在 Kubernetes 中的部署状态是稳定的。
17. 是否监控 Kubernetes 控制平面组件（API Server, Scheduler, Controller Manager, etcd）的健康状况？主要关注哪些指标？
      是的，绝对会监控。对它的监控是我们监控体系中的最高优先级之一。我们通过 kube-prometheus-stack 确保这些组件的 /metrics 端点能被 Prometheus 抓取到。
      我们主要关注以下几个组件的关键指标：
      主要关注的指标：

      API Server:
      延迟：关注请求延迟（P95/P99），通过 API Server 的直方图指标，观察不同操作（如 GET、POST）和资源（如 Pod、Node）的性能。高延迟会影响 kubectl 和控制器。
      错误率：监控 5xx 错误率，任何非零值都表示严重问题。
      QPS：跟踪请求速率，检测异常波动。
      在途请求：监控未处理请求数，高值表明 API Server 过载。

      etcd:
      集群健康：检查是否正常有 Leader（值必须为 1），否则集群可能变为只读。
      Leader 切换：监控切换频率，频繁切换表示网络不稳定。
      gRPC 性能：观察 gRPC 调用的失败率和延迟。
      数据库大小：监控 etcd 存储大小，防止超出配额。

      Scheduler:
      调度延迟：测量 Pod 从创建到调度的端到端时间，高延迟表示调度器瓶颈或资源不足。
      调度失败：记录因资源或规则限制导致的 Pod 调度失败。

      Controller Manager:
      工作队列深度：监控各控制器（如 Deployment、StatefulSet）的队列长度，持续增长表示处理能力不足。
      队列延迟：测量事件在队列中的等待时间，反映控制器性能。
      
19. 监控系统本身的健康状况是如何监控的（"meta-monitoring"）？比如 Prometheus 自身是否健康，Thanos 组件是否正常工作。
      组件存活与健康检查：

      我们利用 Kubernetes 的健康探针（Liveness & Readiness Probes）来监控所有监控组件 Pod 的基本存活状态。例如，我们会为 Prometheus、Alertmanager、Thanos 各组件的 Pod 配置 HTTP 健康检查端点。如果 Pod 变得不健康，Kubernetes 会自动尝试重启它。
      我们配置了 KubeDeploymentReplicasMismatch 告警，确保所有监控组件的 Deployment 的可用副本数都符合预期。例如，如果 Thanos Querier 的两个副本中有一个挂了，这个告警就会触发。
      抓取 Prometheus 自身的指标：

      Prometheus 会暴露大量关于其自身运行状态的指标（在 /metrics 端点）。我们会配置一个 Prometheus 实例去抓取另一个 Prometheus 实例的指标（在 HA 部署中，它们互抓）。
      关键告警规则包括：
      PrometheusConfigurationReloadFailure：Prometheus 加载配置失败。
      PrometheusRuleEvaluationFailures：告警或记录规则评估出错。
      PrometheusTSDBReloadsFailures：头部数据块加载失败。
      PrometheusNotificationsAlertmanagerQueueFull：发送给 Alertmanager 的队列满了，说明 Alertmanager 可能有问题或网络不通。
      监控 Thanos 组件：

      Thanos 的所有组件也都暴露了丰富的自身指标。我们会确保 Prometheus 抓取所有 Thanos 组件的 /metrics 端点。
      关键告警规则包括：
      ThanosQueryFailed：Thanos Querier 执行查询失败。
      ThanosStoreNotReady：Store Gateway 未能成功连接对象存储或加载完索引。
      ThanosCompactFailed：Compactor 执行压缩或降采样任务失败。
      ThanosSidecarBucketOperationsFailed：Sidecar 上传数据块到 MinIO 失败。
      端到端“心跳”告警 (Heartbeat Alert)：

      这是最重要的一环，用于检测整个告警链路是否通畅。
      原理：我们会在 Prometheus 中创建一个特殊的、永远为真的告警规则，我们称之为 Watchdog 或 Heartbeat。

      - alert: Watchdog
      expr: vector(1)
      这个告警会一直处于 Firing 状态，并被发送到 Alertmanager。在 Alertmanager 的路由配置中，我们为这个 Watchdog 告警设置一个特殊的接收器，它可能什么都不做，或者只是记录一个日志。
      然后，我们再创建一个相反的告警规则，它通过 Alertmanager 的 API 查询，检查 Watchdog 告警是否不存在。

      - alert: AlertmanagerFailedToSendAlerts
      expr: absent(ALERTS{alertname="Watchdog"})
      如果 AlertmanagerFailedToSendAlerts 这个告警触发了，就说明从 Prometheus -> Alertmanager -> 最终通知的整条链路中某个环节断了。这个告警我们会用一个完全独立的、最高优先级的渠道（比如直接的电话告警）发送出来。
      通过这套多层次的元监控体系，我们能够确保对监控系统本身的健康状况有清晰的了解，并能在其发生故障时第一时间得到通知。

**刁钻问题 (5)**

第一部分：Prometheus 和 Thanos

1. 当监控数据量巨大时，Thanos Compactor 和 Store Gateway 的性能瓶瓶颈可能出现在哪里？有哪些优化策略？
2. 在金融等对数据准确性要求极高的场景，Prometheus Pull 模式可能存在的微小数据延迟或因目标实例宕机导致的抓取失败（数据丢失），是否会构成问题？你们是如何缓解或应对这种风险的？

第二部分：Prometheus 和 Alertmanager

4. Alertmanager 的高可用集群是如何保证在网络分区（Network Partition）或节点故障情况下，告警状态（如 silences, notifications）的一致性，以及如何确保不丢失、不重复发送告警的？

第三部分：其他

5. 对于需要非常低延迟的告警（例如，几秒内必须响应的交易系统关键故障），当前的 Prometheus -> Alertmanager 架构能否满足？如果不能，有哪些改进方向或替代方案可以考虑？
