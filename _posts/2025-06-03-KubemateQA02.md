---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate QA02            		# 标题 
subtitle:   Kubemate QA02				#副标题
date:       2025-06-03				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
**模块2：监控与告警**

**普通问题 (20)**

第一部分：Prometheus 和 Thanos

1. Prometheus 是如何部署的？采用了哪些高可用配置？
   我们的 Prometheus 采用双副本模式部署以实现高可用。每个 Prometheus 实例都配置了唯一的外部标签，通过标签将 Thanos 进行数据去重。这种架构确保了单个 Prometheus 实例故障不会导致监控数据采集中断。
   为了实现长期存储和全局查询，我们集成了 Thanos。每个 Prometheus Pod 内都运行一个 Thanos Sidecar 容器，它负责两件事：一是将 Prometheus 本地生成的 TSDB 数据块上传到 MinIO 对象存储；二是通过 gRPC 接口向 Thanos Querier 提供该实例内存中的实时数据。
   Thanos Querier 作为统一查询入口，会聚合所有 Sidecar 的实时数据和 Store Gateway 提供的历史数据。它根据外部标签对来自不同副本的数据进行合并去重，从而提供一个统一的全局数据视图。Thanos Store Gateway 组件的作用是连接 MinIO，使其存放的海量历史数据可以被查询。最后，Thanos Compactor 作为一个后台服务，负责对 MinIO 中的数据块进行压缩、降采样和管理数据生命周期，以此来优化存储成本和历史数据查询性能。
2. Thanos 的哪些组件被部署了（如 Sidecar, Querier, Store Gateway, Compactor）？它们各自的作用是什么？具体是如何部署的（不要提供代码）？
   我们部署了 Thanos 的四个核心组件：Sidecar、Querier、Store Gateway 和 Compactor。
   Thanos Sidecar 与 Prometheus 部署在同一个 Pod 中，作为一个伴生容器。这种部署方式使其可以直接访问 Prometheus 的数据卷。它的作用是上传历史数据块到 MinIO，并向 Querier 暴露一个 gRPC 接口以查询该 Prometheus 实例的实时数据。
   Thanos Querier 以无状态、可水平扩展的 Deployment 方式部署，并通过 Service 对外提供查询服务。它的作用是接收 PromQL 查询，然后将请求分发给所有的 Sidecar 和 Store Gateway，最后将返回的结果进行聚合与去重，形成最终的全局数据。
   Thanos Store Gateway 同样以无状态的 Deployment 部署，可以按需扩缩容。它的职责是扫描 MinIO 中的数据块元数据，并向 Querier 提供一个 gRPC 接口，使得对象存储中的历史数据能够被查询。
   Thanos Compactor 作为一个单例组件部署，通常是一个单副本的 Deployment。它的作用是在后台对 MinIO 中的数据进行优化，包括将小数据块合并成大块、对老数据进行降采样以提升长期查询效率，以及执行数据保留策略。
3. MinIO 是如何作为 Thanos 的长期存储的？存储桶的配置和生命周期管理是怎样的？
   minio使用thanos持久化Prometheus数据的工作流程大概是：
   连接配置：
   1：需要与minio交互的thanos组件都通过挂载configmap配置文件存储minio endpoint、访问密钥、私有密钥、存储桶等信息与minio进行通信
   2：数据上传与访问：Thanos Sidecar 会定期将 Prometheus 本地生成的 TSDB 数据块上传到这个指定的 MinIO 存储桶中。而 Thanos Store Gateway 则会监控这个存储桶，将其中数据块的元数据索引在内存中，以便 Thanos Querier 能够查询到这些历史数据。
   关于存储桶的配置和生命周期管理：
   存储桶配置：我们的thanos有专用的存储桶并且关闭了版本控制，因为thanos数据块是不可变的，关闭版本控制可以避免不必要的开销，这个存储桶仅允许thanos所在的命名空间进行读写操作
   生命周期管理：我们没有使用minio自身的生命周期管理策略来管理监控数据，我们通过Thanos Compactor 来执行数据保留，我们的保留策略是原始精度数据 14 天，5 分钟降采样数据 90 天，1 小时降采样数据 1 年，Compactor 根据这些规则自动删除minio中过期的符合条件的数据块
   这种方式将数据管理的逻辑保留在 Thanos 层，与底层存储解耦，逻辑更清晰，也更容易适配不同的对象存储后端。
4. Prometheus 的抓取配置 (scrape configs) 是如何管理的？是否支持动态更新？
   我们的抓取配置规则是通过Prometheus operator的Service Monitor和Pod Monitor对应的CRD来实现的，我们平台会在每个命名空间创建的时候同步创建Service Monitor和Pod Monitor对应的CR，并且在像deploy、statefulset、daemonset、service这类资源被创建的时候同步打上命名空间对应的标签，即可被Prometheus operator自动采集，这样做可以避免不同租户之间互相干扰，也可以避免错误数据影响范围扩大。
5. 监控数据的查询性能如何？特别是在查询大范围时间序列数据时，Thanos 的降采样（down-sampling）是如何帮助提升性能的？
   短期查询和长期查询性能是不同的：
   短期查询（例如，过去几小时）：性能非常好。这类查询主要由 Thanos Querier 直接转发给 Thanos Sidecar，数据直接从 Prometheus 实例的内存中获取，响应速度非常快，通常在毫秒到秒级。
   长期查询（例如，过去几个月）：性能稍微慢一些，但是不是特别慢，大概在秒级别，3到5秒左右，因为 thanos compactor 会定期扫描原始数据块，在数据块达到规定的阈值时触发降采样，将原始数据计算出精度更低的版本进行存储，我们的规则的原始数据14天，5分钟降采样90天，1小时降采样一年。并且thanos querier受到查询时会分析查询的范围和步长，如果用户查询的是过去 30 天的数据，Querier 会自动选择使用 5 分钟或 1 小时分辨率的数据来响应，而不是去读取海量的原始数据。这样长期查询的性能可以提升到秒级
6. 什么是Prometheus指标的基数？你们是如何管理 Prometheus 指标的基数（Cardinality）问题的？有没有遇到过高基数带来的性能问题，又是如何解决的？
   我来解释一下这个问题，假如你有两个接口/metrics/AAA和/metrics/BBB，Prometheus采集之后就是两个时间序列，就是两个基数，假如/metrics/AAA这个接口有动态参数/metrics/AAA？userid=123456,这就会导致/metrics/AAA这个接口有多少动态参数请求就有多少时间序列，从而有多少基数，基数越大查询效率越低从而导致性能问题。我们也遇见过应用高基数带来的性能问题，这几乎是不可避免的，至于解决方案主要有以下几点：1、首先就是与开发团队沟通，尽量避免使用动态参数作为url，从而从根源上避免超高基数的产生。2、第二点就是聚合标签，如果我们不关心高基数的具体细节，只关心其聚合值。我们可以修改Service Montior从而修改配置记录规则（Recording Rules），预先计算好聚合后的指标，然后使用 labeldrop 丢弃原始的高基数指标，只保留聚合后的低基数指标。
7. Thanos Querier 是如何对来自两个 Prometheus 副本的数据进行去重的？这个机制的原理是什么？
   这个原理其实非常简单，就是为每个Prometheus副本分别配置了不同的prometheus_replica这样的外部标签，采集的时候就会采集到两个几乎一样的时间序列，区别就是它们两个prometheus_replica这个外部标签不同，假如一共叫做prometheus_replica=“A”另一个叫做prometheus_replica=“B”，查询聚合的时候， Thanos Querier 发起一个 PromQL 查询时会查询到标签不同的两组结果，Thanos Querier会使用去重功能，比如两个序列都作为副本序列，Thanos Querier会选择哈希值最大的副本数据，确保在多次查询中结果一直且为一条连续时间序列。从而实现去重。
   （通过 --query.replica-label 参数指定要去重的标签，这里就是 prometheus_replica）
8. 你们为 Prometheus 和 Thanos 的各个组件设置了怎样的资源请求（requests）和限制（limits）？在资源规划上有什么考量？
   Prometheus + Thanos Sidecar：
   CPU: requests 较高，limits 更高。Prometheus 的抓取和规则评估是 CPU 密集型的。
   Memory: requests 和 limits 都设置得很高，并且通常相等。这是因为 Prometheus 的内存使用量与指标基数和抓取的目标数量强相关。我们不希望它因为内存 limits 被 OOMKilled，这会导致 2 小时内的监控数据丢失。我们会根据实际的基数来设定一个充裕的内存值，比如 32Gi 或更高。Sidecar 本身内存消耗不大。
   Thanos Querier：
   CPU: requests 中等，limits 较高。查询的复杂度和并发度会消耗大量 CPU。
   Memory: requests 和 limits 都比较高。Querier 需要在内存中处理从 Sidecar 和 Store Gateway 返回的数据，复杂的聚合查询会消耗大量内存。它是一个无状态组件，可以水平扩展，所以我们会部署多个副本分担压力。
   Thanos Store Gateway：
   CPU: requests 较低，limits 中等。它的主要开销是响应 gRPC 请求，CPU 消耗通常不大。
   Memory: requests 和 limits 非常高。Store Gateway 启动时需要加载 MinIO 中所有数据块的元数据（索引和标签信息）到内存中。这个内存量与对象存储中数据块的总量成正比。这是我们监控的重点，如果内存持续增长，说明需要考虑优化或分片部署。
   Thanos Compactor：
   CPU & Memory: requests 中等，limits 较高。Compactor 在执行压缩和降采样任务时是资源密集型的，尤其是在处理大数据块时。但这些任务是周期性的，不是持续的。所以我们给它较高的 limits 以应对峰值使用，但 requests 可以相对保守。
   
第二部分：Prometheus 和 Alertmanager

1. Alertmanager 支持哪些告警渠道？邮件和短信告警是如何配置的？告警的原理是什么？
2. 平台预定义了哪些关键的告警规则？用户是否可以自定义告警规则？如果可以，是如何实现的？
3. 告警的抑制 (Silencing) 和去重 (Deduplication) 是如何通过 Alertmanager 实现的？请举例说明。
4. 告警通知中包含哪些关键信息，以帮助用户快速定位问题？模板是如何定制的？
5. 请描述一个告警从 Prometheus 触发到通过 Alertmanager 发送到用户手中的完整生命周期。
6. 你们如何处理告警风暴（Alert Storm）和告警抖动（Flapping）的问题？
7. Alertmanager 的路由（routing）规则是如何配置的？能否举一个例子，比如将不同严重程度或不同业务线的告警发送给不同的人？

第三部分：其他

1. 应用健康状态是如何定义的？通过哪些指标来衡量？
2. 用户如何通过仪表盘下钻 (drill down) 查看特定应用或组件的详细监控数据？
3. 是否监控 Kubernetes 控制平面组件（API Server, Scheduler, Controller Manager, etcd）的健康状况？主要关注哪些指标？
4. 应用性能指标（如 QPS, 延迟, 错误率）是如何采集和展示的？这部分是基于 OpenTelemetry 吗？
5. 监控系统本身的健康状况是如何监控的（"meta-monitoring"）？比如 Prometheus 自身是否健康，Thanos 组件是否正常工作。

**刁钻问题 (5)**

第一部分：Prometheus 和 Thanos

1. 当监控数据量巨大时，Thanos Compactor 和 Store Gateway 的性能瓶瓶颈可能出现在哪里？有哪些优化策略？
2. 如何处理 Prometheus 实例间的抓取目标冲突或重复抓取问题，尤其是在多集群联邦查询的场景下？
3. 在金融等对数据准确性要求极高的场景，Prometheus Pull 模式可能存在的微小数据延迟或因目标实例宕机导致的抓取失败（数据丢失），是否会构成问题？你们是如何缓解或应对这种风险的？

第二部分：Prometheus 和 Alertmanager

4. Alertmanager 的高可用集群是如何保证在网络分区（Network Partition）或节点故障情况下，告警状态（如 silences, notifications）的一致性，以及如何确保不丢失、不重复发送告警的？

第三部分：其他

5. 对于需要非常低延迟的告警（例如，几秒内必须响应的交易系统关键故障），当前的 Prometheus -> Alertmanager 架构能否满足？如果不能，有哪些改进方向或替代方案可以考虑？
