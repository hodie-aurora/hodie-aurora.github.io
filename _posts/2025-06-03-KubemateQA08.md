---
layout:     post   				# 使用的布局（不需要改）
title:      Kubemate QA08            		# 标题 
subtitle:   Kubemate QA08				#副标题
date:       2025-06-03				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Kubemate
---
**模块8：AI 基础设施（基础版）**

### 基础问题 (20)

**1. 集成与部署：请描述一下你们是如何通过 Go 后端将 Ollama 服务作为一个 Kubernetes 资源进行管理的？是使用了自定义控制器（Operator）来封装其生命周期，还是简单的 Deployment/StatefulSet 配置？为什么选择这种方式？**

我们选择使用标准的 Kubernetes `Deployment` 来管理 Ollama 服务。

* **开发实现 (Go)** ：我们的 Go 后端服务并不直接控制 Pod 的生命周期，而是通过 Kubernetes 的 `client-go` 库与 K8s API Server 交互。当用户需要部署一个新的模型服务时，我们的后端会根据预定义的模板动态生成一个 `Deployment` 的 YAML 清单。这个清单会指定 Ollama 的容器镜像、启动时需要加载的模型（通过容器的 `args` 传入），以及 CPU/内存/GPU 的资源请求（`resources`）。然后，后端将此清单提交给 API Server，由 Kubernetes 内置的 Deployment Controller 负责完成后续的部署和状态维持。
* **运维保障** ：之所以选择 `Deployment` 而不是自己开发控制器，主要是为了 **降低复杂性和维护成本** 。`Deployment` 提供了滚动更新、副本数量管理和自愈能力，完全满足了基础版对无状态推理服务的需求。这让我们能专注于上层业务逻辑，而不是重新发明轮子，同时也保证了系统的稳定性和可预测性。

**2. 异步任务处理：用户上传文档后的处理流程（拆分、向量化、存储）是同步执行还是异步执行？如果是异步，你们使用了什么技术栈来调度和追踪这些后台任务的状态？**

这个流程是**异步执行**的。

* **开发实现 (Go)** ：用户通过前端上传文档后，我们的 Gin 后端会立即将原始文件存入 MinIO，并返回一个任务 ID 给用户，前端可以据此轮询状态。同时，后端会为这个文档处理任务创建一个 Kubernetes `Job`。我们会动态生成 `Job` 的配置，其中包含一个专用的处理容器，该容器会从 MinIO 拉取文档、执行拆分和向量化，最后将结果写入 Milvus。
* **运维保障** ：使用 K8s `Job` 的好处是它天生就是为处理批处理任务设计的。Kubernetes 会确保 `Job` 最终成功执行完成。我们的 Go 后端服务会定期通过 K8s API 查询这个 `Job` 的状态（`Active`, `Succeeded`, `Failed`），并更新数据库中该任务的状态，从而反馈给前端。这种方式利用了 K8s 的原生能力，可靠性高，且易于监控和排查。

**3. RAG 核心逻辑：请详细拆解 RAG 的检索生成流程。当用户提问时，系统是如何构建最终的 Prompt 的？**

整个 RAG 流程由我们的 Go 后端服务统一编排：

1. **查询向量化** ：当接收到用户的提问（Query）后，后端首先调用一个专用的 Embedding 模型服务（通常也是一个 Ollama 实例），将用户的提问转换成一个查询向量。
2. **向量检索** ：后端服务拿着这个查询向量去请求 Milvus，执行近似最近邻搜索，从指定的数据集合中检索出 Top-K 个最相关的文档片段（Chunks）。
3. **构建 Prompt** ：这是核心步骤。我们会根据一个预设的模板来构建最终提交给大模型的 Prompt。一个典型的模板如下：

```
   "基于以下已知信息，请简洁、准确地回答用户的问题。如果信息不足，请回答'根据已知信息，我无法回答该问题'。

   [已知信息]
   ---
   {context_chunk_1}
   ---
   {context_chunk_2}
   ---
   ...

   [用户问题]
   {user_query}
   "
```

   我们会将从 Milvus 检索到的文档片段填充到 `{context_chunk}` 部分，将用户的原始问题填充到 `{user_query}`。

1. **生成答案** ：最后，将这个构建好的完整 Prompt 发送给 Ollama 中部署的大语言模型，获取最终答案并返回给用户。我们还会将用户的历史对话摘要加入到上下文中，以支持多轮对话。

**4. 向量数据库设计：在设计 Milvus 的集合（Collection）时，你们主要考虑了哪些字段？**

在设计 Milvus 集合时，我们不仅存储了向量，还包含了丰富的元数据以支持复杂业务需求：

* `vector`: 存储文档片段的嵌入向量，用于相似度检索。
* `chunk_text`: 存储原始的文档片段文本，用于在生成 Prompt 时提供上下文。
* `document_id`: 关联该片段所属的原始文档，方便溯源。
* `tenant_id`: 用于多租户数据隔离的关键字段，确保查询时只能检索到属于当前租户的数据。
* `chunk_id`: 每个片段的唯一标识符，用于精确定位和管理。
* `source`: 标记文档来源，例如 "研报"、"财报" 等，支持按类别过滤。

**5. 对象存储策略：MinIO 在 AI 基础版模块中具体存储了哪几类数据，并说明为什么选择 MinIO 而不是直接使用 Kubernetes 的持久化存储（PVC）？**

MinIO 在此模块中存储三类核心数据：

1. **原始文档** ：用户上传的未处理的原始文件（如 PDF, Word, TXT）。
2. **数据集与日志** ：在处理过程中生成的中间文件或日志，用于调试和审计。
3. **模型文件** ：对于私有化部署，客户可以上传自己的模型文件到 MinIO，供 Ollama 加载。

选择 MinIO 而不是 PVC 的原因：

* **访问模式** ：AI 处理流水线中的多个服务（Go 后端、K8s Job 里的处理脚本、Ollama）都需要访问这些文件。使用 S3 协议的 MinIO，任何服务都可以通过 API 方便地访问，而 PVC 通常只能被挂载它的 Pod 访问，共享不便。
* **可扩展性** ：MinIO 本身支持横向扩展，能够轻松应对海量非结构化数据的存储需求，而 PVC 的容量和性能通常受限于底层的存储类别（StorageClass）。
* **生态系统** ：S3 是对象存储的事实标准，生态成熟，便于与其他云原生工具集成。

**6. 缓存应用：你提到了 Redis 用于高性能缓存。请问它具体缓存了哪些数据？是缓存高频查询的向量检索结果，还是缓存大模型生成的最终答案？缓存的失效策略是怎样的？**

是的，Redis 在我们的架构中主要用于两类缓存：

1. **高频查询的最终答案** ：对于完全相同的用户提问，我们会直接缓存由 LLM 生成的最终答案。这是一个精确匹配缓存。
2. **Milvus 检索结果** ：我们也会缓存用户查询向量与从 Milvus 检索到的文档片段 ID 列表的映射关系。这样，如果后续有语义相似的查询，可以跳过部分检索步骤。

 **缓存策略** ：我们主要使用 **TTL（Time-To-Live）** 结合 **LRU（Least Recently Used）** 策略。对于最终答案，我们会设置一个较短的 TTL（例如 1 小时），因为信息可能过时。对于检索结果，TTL 可以设置得更长。当缓存达到内存上限时，Redis 会自动驱逐最近最少使用的数据。

**7. 监控与可观测性：你们是如何通过 Prometheus 监控 Ollama 服务的？是否自定义了业务指标？**

是的，我们对 Ollama 和整个 RAG 流程进行了深度监控：

* **基础组件监控** ：Ollama 自身暴露了 `/metrics` 端点，我们通过 Prometheus 的静态配置或 Service Discovery 来抓取这些原生指标，如模型加载时间、GPU 内存使用等。
* **自定义业务指标** ：这是关键。在我们的 Go 后端服务中，我们使用了 Prometheus 的 Go 客户端库，埋点了一系列关键业务指标：
* `rag_requests_total` (Counter): 按模型、按租户统计 RAG 请求总数。
* `rag_request_duration_seconds` (Histogram): 测量从接收用户请求到返回最终答案的端到端延迟，可以计算 P95/P99。
* `vector_search_duration_seconds` (Histogram): 专门测量 Milvus 向量检索的耗时。
* `document_processing_status` (Counter): 统计文档处理流水线中成功和失败的次数。

这些自定义指标是我们在 Vue3 仪表盘上展示 AI 任务性能和健康状况的核心数据来源。

**8. GPU 资源管理：Ollama 服务对资源（特别是 GPU）非常敏感。你们是如何通过 Kubernetes 进行资源声明、调度和隔离的？**

我们通过 Kubernetes 原生的设备插件（Device Plugin）机制来管理 GPU。

* **运维保障** ：首先，在集群的 GPU 节点上，我们部署了  **NVIDIA Device Plugin for Kubernetes** 。这个插件会发现节点上的 GPU 并向 K8s API Server 注册这些 GPU 资源。
* **开发实现** ：在我们的 Go 后端生成 Ollama 的 `Deployment` 清单时，会在容器的 `resources` 字段中明确声明对 GPU 的需求，例如：

```yaml
  resources:
    limits:
      nvidia.com/gpu: 1
```

  这样，Kubernetes 调度器在调度这个 Pod 时，就会自动寻找一个拥有可用 GPU 资源的节点。这确保了 GPU 资源被正确分配和隔离，避免了资源争抢。

**9. 模型生命周期管理：Kubemate 是如何管理 Ollama 中运行的模型的？用户是否可以通过平台界面拉取新模型？**

是的，用户可以通过平台管理模型。

* **开发实现 (Go & Vue3)** ：前端 Vue3 界面提供了一个模型管理页面，用户可以输入模型的名称（如 `llama3`）。点击“拉取”后，请求会发送到我们的 Go 后端。
* 后端逻辑分为两步：
  1. **与 Ollama 交互** ：后端会调用 Ollama 的 `/api/pull` REST API，命令 Ollama 实例开始从其配置的源（如 Docker Hub）拉取模型。这是一个异步过程。
  2. **状态追踪** ：后端会轮询 Ollama 的 `/api/ps` 或 `/api/tags` 接口，检查模型是否已经成功下载并可用。这个状态会实时或准实时地同步到前端界面。
* 整个过程对用户是透明的，他们只需要在界面上操作，后端 Go 服务负责了所有与 Ollama API 的交互。

**10. 容错与告警：在文档处理的流水线中，如果某个环节失败，系统的容错和重试机制是怎样的？**

我们的容错和告警机制是多层次的：

* **容错与重试** ：我们使用的 Kubernetes `Job` 具有内置的重试机制。在 `Job` 的配置中，我们可以设置 `backoffLimit`，允许 Pod 在失败后自动重启和重试一定的次数。对于网络抖动或临时性的服务不可用（如 Milvus 瞬断），这种机制非常有效。
* **告警** ：
* **Job 失败告警** ：我们配置了 Alertmanager 规则，当一个 K8s `Job` 的失败次数超过阈值（例如 `backoffLimit` 次后仍为 `Failed` 状态），就会触发告警，通知运维团队。
* **应用层告警** ：在我们的 Go 处理程序中，我们对关键步骤（如向量化、写入 Milvus）都做了 `try-catch`。一旦捕获到无法通过重试解决的错误（如文档格式不支持、Milvus schema 冲突），我们会将任务状态标记为永久失败，并主动推送一个高优先级的告警到 Alertmanager，告警信息会包含详细的错误日志和任务 ID，便于快速定位。

**11. 文档预处理：你们默认的拆分策略是什么？这个策略是否可以配置？**

我们默认采用的是**递归字符拆分（Recursive Character Text Splitter）** 策略。

* **策略描述** ：这种策略会尝试按一系列分隔符（首先是 `\n\n`，然后是 `\n`，然后是空格）来拆分文本，以尽可能地保持段落和句子的完整性。我们设置了 `chunk_size`（例如 512 个 token）和 `chunk_overlap`（例如 64 个 token），重叠部分有助于在检索时保留上下文的连续性。
* **可配置性** ：在基础版中，这个策略是固定的，以简化用户操作。但在我们的设计中，处理脚本的这部分是模块化的。在高级版或针对特定客户的部署中，我们可以通过 `ConfigMap` 或环境变量的方式，让用户选择不同的拆分策略（如按 Markdown 标题、按固定 token 数）或调整 `chunk_size` 和 `chunk_overlap` 参数。

**12. 模型灵活性：系统默认使用哪个 Embedding 模型？是否支持客户替换？**

* **默认模型** ：我们默认使用一个社区公认的、性能均衡且支持中英双语的开源 Embedding 模型，例如 `bge-base-zh-v1.5`，通过 Ollama 进行部署。
* **替换支持** ：是的，我们支持客户替换。替换流程设计得尽可能简单：

1. **上传模型** ：如果客户有私有的 Embedding 模型（通常是 ONNX 或 GGUF 格式），他们可以将其上传到平台的 MinIO 存储中。
2. **配置更新** ：在平台的管理界面上，用户可以指定一个新的 Embedding 模型服务。我们的 Go 后端会更新相应的 `Deployment` 配置，将其指向新的模型文件或镜像。
3. **重新向量化** ：替换模型后，我们会提示用户，旧的向量数据已经失效，并提供一个选项，让他们可以批量触发一个后台任务，用新的 Embedding 模型重新处理所有相关的文档。

**13. API 安全：RAG 的推理 API 是如何暴露给用户的？如何实现认证、鉴权和速率限制？**

所有对外的 API 都通过 **Traefik 网关**进行统一管理。

* **认证与鉴权** ：用户的请求首先到达 Traefik。我们利用 Traefik 的中间件（Middleware）功能。请求会先经过一个认证中间件，该中间件会调用我们的 Go 用户服务来验证请求头中的 JWT (JSON Web Token)。验证通过后，我们还会结合 Kubernetes 的 RBAC，确保该用户有权限访问其请求的资源（例如，只能查询自己租户的数据）。
* **速率限制** ：同样在 Traefik层面实现。我们可以为 API 接口配置速率限制中间件，定义每个用户或每个 IP 在单位时间内的最大请求数，有效防止恶意攻击或服务滥用。

**14. 向量检索调优：你们是如何配置 Milvus 的索引类型和搜索参数的？**

* **索引类型** ：对于大部分场景，我们默认使用 **HNSW (Hierarchical Navigable Small World)** 索引。因为它在检索速度和准确率之间取得了很好的平衡，并且不需要像 IVF 系列索引那样进行“训练”，非常适合数据动态变化的场景。
* **搜索参数** ：在 Go 后端进行查询时，我们会设置关键的搜索参数。例如，`ef`（搜索时遍历的图的范围大小）是一个重要的调优参数。我们会提供一个默认值（如 64），同时允许高级用户在发起查询时通过 API 参数进行调整，以在速度和精度之间做权衡。这些配置是根据我们的压测经验得出的最佳实践。

**15. 备份与恢复：请说明你们为 MinIO 和 Milvus 设计的备份与恢复方案。**

* **MinIO 备份** ：我们使用 MinIO 自带的客户端工具 `mc`。通过一个 Kubernetes `CronJob`，我们定期执行 `mc mirror` 命令，将生产环境 MinIO 存储桶中的数据（原始文档、模型文件）单向同步到一个独立的、用于备份的 MinIO 实例或兼容 S3 的云存储上。备份频率通常是每天一次。
* **Milvus 备份** ：Milvus 的备份相对复杂。我们同样使用 `CronJob`，定期调用 Milvus 的备份工具或 API 来创建数据快照。这些快照文件本身也存储在 MinIO 中。
* **恢复目标 (RTO)** ：我们的 RTO 目标是 4 小时内。恢复流程是标准化的，有详细的文档。运维人员可以根据文档，从备份实例中恢复 MinIO 数据，并使用 Milvus 的快照进行数据恢复。

**16. 性能压测：你们是如何对 AI 基础版进行压力测试的？关注的核心性能指标有哪些？**

我们使用 `k6` 或 `JMeter` 这类工具，通过编写脚本来模拟真实的用户行为，进行压力测试。

* **测试场景** ：

1. **并发推理** ：模拟大量用户同时提问。
2. **文档注入** ：模拟大量用户同时上传文档。

* **核心性能指标 (KPIs)** ：
* **并发用户数 (Concurrent Users)** ：系统在不出现性能显著下降的情况下能支持的最大并发请求数。
* **QPS (Queries Per Second)** ：推理接口每秒能处理的查询次数。
* **P95/P99 推理延迟** ：95% 和 99% 的请求的端到端响应时间。这是衡量用户体验的关键。
* **文档处理吞吐量** ：单位时间内系统能成功处理（拆分、向量化、入库）的文档数量。
* **资源利用率** ：在压力下，CPU、内存、GPU 的使用率，以评估资源配置是否合理。

**17. 多租户数据隔离：如何确保租户 A 的文档数据和向量数据与租户 B 的数据是严格隔离的？**

我们采用**多层隔离**策略：

1. **逻辑隔离 (应用层)** ：这是最核心的隔离。所有 API 请求都必须携带租户信息（通过 JWT 解析）。我们的 Go 后端在处理任何数据时，都会强制加上 `WHERE tenant_id = '...'` 这样的过滤条件。无论是查询 MinIO 的文件列表，还是查询 Milvus 的向量，这都是第一道防线。
2. **Milvus 分区 (Partition)** ：为了提升性能和管理性，我们在 Milvus 的一个 Collection 内部为每个租户创建一个独立的 Partition。这样在查询时，可以直接指定在某个租户的 Partition 内搜索，物理上将数据分组，隔离性更强，查询效率也更高。
3. **Kubernetes 命名空间 (Namespace)** ：对于有更高隔离要求的金融客户，我们会为每个租户部署一套独立的 AI 基础设施（Ollama, Milvus 等），这些资源都运行在租户专属的 Kubernetes Namespace 中，通过 RBAC 规则实现网络和资源的硬隔离。

**18. 内部网络安全：描述一下组件在 Kubernetes 集群内部的网络通信模式。是否利用了 Network Policies？**

是的，我们利用 **Kubernetes Network Policies** 来实现“零信任”网络。

* **通信模式** ：
* Go 后端服务需要访问 Ollama、Milvus、MinIO 和 K8s API Server。
* 处理文档的 K8s `Job` 需要访问 MinIO 和 Milvus。
* Ollama、Milvus、MinIO 之间通常不需要互相通信。
* **策略实现** ：
* 我们默认设置一个 `deny-all` 的 Ingress 策略，禁止所有进入 Pod 的流量。
* 然后，我们为每个组件（如 Milvus）定义精细的 `NetworkPolicy`，明确只允许带有特定标签的 Pod（如我们的 Go 后端服务 Pod）访问它的特定端口（如 Milvus 的 19530 端口）。
* 这样，即使某个服务被攻破，它也无法在集群内部横向移动去访问未授权的其他服务，极大地提升了内部网络的安全性。

**19. 成本与资源计量：在多租户场景下，平台是否具备计量每个租户 AI 资源消耗的能力？**

是的，我们实现了基础的资源计量。

* **技术原理** ：我们的计量主要基于 **应用层日志和监控指标** 。
* **模型调用次数** ：Go 后端在处理每次 API 请求时，都会记录日志，其中包含 `tenant_id` 和调用的 `model_name`。我们可以通过分析这些日志来统计调用次数。
* **文档处理量** ：同样，文档处理任务的日志也包含了 `tenant_id` 和文档大小，可以据此统计。
* **GPU 使用** ：这是一个挑战。我们无法做到精确的“GPU 秒”计量，但我们通过 Prometheus 监控每个租户专用的 Ollama `Deployment` 的 GPU 使用率，可以得出一个近似的资源消耗概览。
* 这些数据会被汇总到一个内部的数据库中，用于生成租户的资源使用报表，为成本分摊提供依据。

**20. 用户体验反馈：从用户角度看，当他们上传一篇大型文档后，系统是如何反馈处理进度的？**

我们通过**前端轮询 + 后端状态机**的方式提供实时反馈。

* **后端状态机** ：在我们的 Go 服务中，每个文档处理任务都有一个状态（`PENDING`, `PROCESSING`, `VECTORIZING`, `COMPLETED`, `FAILED`）。当 K8s `Job` 开始执行、完成向量化、最终结束时，它会通过回调或状态更新机制通知主服务更新任务状态。
* **前端轮询** ：用户上传文档后，前端会拿到一个任务 ID。随后，前端会每隔几秒钟向后端的一个 `/task-status/{id}` 接口发起请求。
* **界面展示** ：后端返回任务的当前状态后，前端界面会相应地显示一个进度条或状态文本，例如“处理中：正在拆分文档...”、“处理中：正在生成向量...”、“处理完成”或“处理失败，请联系管理员”。这为用户提供了清晰、及时的反馈，避免了长时间的未知等待。

---

### 深入及挑战性问题 (5)

**1. 技术选型局限性：Ollama 是否会成为瓶颈？你认为它的天花板在哪里？有没有考虑过替代方案？**

这是一个非常好的问题，我们内部也深入讨论过。Ollama 的确存在天花板，尤其是在我们服务的金融机构生产环境中。

* **瓶颈分析** ：

1. **并发性能** ：Ollama 的设计哲学是简单易用，它在底层的批处理（Batching）和请求调度方面不如专门的推理服务器。当面临大规模并发请求时，其吞吐量（QPS）会很快达到瓶颈，请求延迟也会急剧上升。
2. **模型管理** ：它缺乏企业级场景需要的高级功能，如模型版本控制、A/B 测试、Canary 部署。我们目前在 Go 应用层做了简单的管理，但这远非一个健壮的 MLOps 解决方案。
3. **异构硬件支持** ：虽然支持 GPU，但对于更复杂的硬件加速（如多 GPU 推理、张量并行）或特定加速器（如 Google TPU、AWS Inferentia），Ollama 的支持是有限的。

* **替代方案** ：是的，我们已经将 **NVIDIA Triton Inference Server** 和 **vLLM** 作为高级版的备选方案。
* **Triton** 是一个功能全面的推理服务器，支持动态批处理、多模型部署、性能分析，并且对 NVIDIA 硬件有深度优化，非常适合需要高吞吐和稳定性的严肃生产环境。
* **vLLM** 则是一个专注于 LLM 推理性能的库，其核心技术 PagedAttention 能极大地提升吞吐量。
* 我们的策略是，**基础版使用 Ollama 满足快速上手和中低负载场景，当客户有更高的性能和 MLOps 需求时，可以平滑升级到由 Triton 或 vLLM 支持的高级版 AI 基础设施。**

**2. RAG 效果的深层优化：当遇到检索结果相关性差或内容冲突时，你们的系统有何优化策略？**

我们意识到了朴素 RAG 的局限性，并规划了几个深层优化策略：

1. **查询重写 (Query Rewriting)** ：对于口语化或模糊的用户提问，我们可以在检索前增加一个“预处理”步骤。使用一个小型、快速的 LLM（例如 Phi-3-mini）对原始问题进行重写，生成一个更结构化、更适合向量检索的查询语句。这能显著提升检索召回率。
2. **重排序 (Re-ranking)** ：从 Milvus 召回的 Top-K（例如 K=20）个结果，在语义上可能接近，但与问题的直接相关性有差异。我们计划引入一个轻量级的交叉编码器（Cross-Encoder）模型。它会逐一评估“用户问题”和每个“召回的文档块”的精确相关性得分，然后对这 20 个结果进行重排序，只将最相关的 Top-N（例如 N=5）个结果送入最终的 LLM。
3. **生成阶段的判断** ：在构建最终 Prompt 时，我们会加入特定的指令，引导 LLM 进行批判性思考。例如，我们会指示模型：“请基于提供的上下文回答问题。如果上下文之间存在矛盾，请指出矛盾点。如果上下文无法回答问题，请明确说明。” 这赋予了模型一定的“事实核查”能力。

这些优化策略目前正在高级版中进行实验，成熟后可以下沉到基础版。

**3. 向量索引的智能化：Kubemate 是如何帮助用户做出正确选择的？**

这是一个非常实际的运维开发问题。我们通过**“默认最优 + 配置模板”**的方式来解决。

* **默认最优** ：对于 90% 的用户，我们默认使用 HNSW 索引。我们根据大量的测试，为不同规模的数据量（例如，10 万、100 万、1000 万向量）预设了一套我们认为是最佳实践的 HNSW 参数（如 `M` 和 `efConstruction`）。用户创建集合时，系统会自动应用这套配置，开箱即用。
* **配置模板** ：对于有特殊需求的用户，我们在前端界面提供了几种预设的“性能模板”：
* **“最高速检索”模板** ：使用 HNSW，并设置较高的 `ef` 搜索参数，牺牲一些内存换取最低的查询延迟。
* **“内存优化”模板** ：推荐使用 IVF_FLAT 索引，适合超大规模但对实时性要求不高的场景。系统会引导用户完成“训练”索引的步骤。
* **“最高精度”模板** ：推荐使用 FLAT 索引（暴力搜索），仅用于小规模数据集的精确验证场景。

我们的 Go 后端会根据用户选择的模板，生成并应用对应的 Milvus 索引配置。我们不期望用户去理解复杂的索引参数，而是让他们从业务需求出发做选择。

**4. 规模化挑战：设想一个场景，某个金融客户需要处理数千万份研报，向量数据达到 TB 级别。你认为当前“基础版”的架构会首先在哪个组件上遇到瓶颈？你会提出怎样的架构演进方案？**

这个场景下，当前基础版架构的瓶颈会非常明显，我认为瓶颈出现的顺序是：

1. **Milvus** ：单机版的 Milvus 无法支撑 TB 级的向量数据和高并发查询。它会首先在**内存和磁盘 I/O** 上达到极限。
2. **文档处理流水线** ：我们目前“一个文档一个 K8s Job”的模式，在处理数千万份文档时，会瞬间创建海量的 Job 对象，给 Kubernetes API Server 带来巨大压力，甚至可能导致其崩溃。
3. **Ollama** ：当查询并发量上来后，Ollama 的推理性能会成为第三个瓶颈。

 **架构演进方案** ：

* **Milvus 演进** ：必须从单机版切换到  **Milvus 分布式集群** 。我们会部署其完整的微服务架构，包括 Proxy、Query Node、Index Node 和 Data Node，并将它们横向扩展。数据会被分片（Sharding）存储在多个 Data Node 上，查询压力也会由多个 Query Node 分担。
* **处理流水线演进** ：我们会放弃 K8s Job 模式，转而采用 **基于消息队列的生产者-消费者模式** 。
* Go 后端接收到文档后，不再创建 Job，而是将一个处理任务的消息（包含文档在 MinIO 中的路径和租户信息）发送到  **Redis Stream 或 RabbitMQ** 。
* 我们会部署一个常驻的、可水平扩展的 `Deployment`，其中的 Pod 就是消费者（Worker）。这些 Worker 会从消息队列中拉取任务，执行处理，并将结果写入 Milvus 集群。这种架构的吞吐能力和弹性都远超 Job 模式。
* **推理服务演进** ：将 Ollama 替换为  **Triton Inference Server 或 vLLM** ，并部署多个副本，通过 Kubernetes HPA（Horizontal Pod Autoscaler）根据 CPU/GPU 负载自动扩缩容。

**5. 私有化部署的灵活性：如何解决模型的来源问题？如何接入异构模型服务？**

这是私有化部署的核心痛点，我们的设计思路是**“解耦”和“抽象”**。

* **解决模型来源问题** ：

1. **内部模型仓库** ：我们会将 **MinIO** 作为内部的模型仓库。客户可以将他们获准使用的模型文件（GGUF、ONNX 等格式）上传到指定的 MinIO 存储桶中。
2. **启动时加载** ：在部署 Ollama 或其他推理服务时，我们会通过 `initContainer` 或修改容器的启动命令，让服务在启动时从 MinIO 中拉取模型文件到本地，而不是从公共互联网下载。这确保了整个环境的离线运行。

* **接入异构模型服务的设计思路** ：

  为了接入 API 不同的私有模型，我们需要在 Go 后端引入一个 **适配器层（Adapter Layer）** 。

1. **定义标准接口** ：首先，在 Go 代码中定义一个统一的 `InferenceInterface` 接口，它包含我们业务所需的核心方法，如：
   ``go type InferenceInterface interface { Generate(prompt string, options GenerationOptions) (string, error) GetEmbeddings(texts []string) ([][]float32, error) } ``
2. **创建具体适配器** ：
   * 我们会有一个 `OllamaAdapter`，它实现了 `InferenceInterface` 接口，其内部逻辑是调用 Ollama 的 REST API。
   * 当需要接入客户的自研模型时，我们只需要为其开发一个新的适配器，例如 `CustomModelAdapter`。这个适配器也实现 `InferenceInterface` 接口，但其内部逻辑是按照客户私有模型的 API 规范来构造和发送请求。
3. **动态加载** ：在我们的主应用中，所有调用模型的地方都只依赖于 `InferenceInterface` 接口。系统启动时，会根据配置文件中的一个 `model_provider` 字段（例如 `ollama` 或 `custom_financial_model`）来实例化并注入具体的适配器。

  通过这种方式，我们的核心业务逻辑与具体的模型服务实现完全解耦，接入新的模型服务只需要开发一个新的、遵循统一接口的适配器即可，平台具备了极高的灵活性和扩展性。
