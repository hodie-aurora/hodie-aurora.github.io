---
layout:     post   				# 使用的布局（不需要改）
title:      AAll		# 标题 
subtitle:   AAll					#副标题
date:       2025-10-26				# 时间
author:     zhaohaiwen 				# 作者
header-img: img/post-bg-2025-01-07.jpg		#这篇文章标题背景图片
catalog: true 					# 是否归档
tags:						#标签
    - Fast
---
### GoLang

#### GPM模型

我来解释一下什么是GPM模型，首先G就是goroutine，也就是协程、每个只占几kb，非常轻量级，而p是逻辑处理器，每个逻辑处理器对应一个本地队列，M是内核线程

当协程被创建的时候，会放入逻辑处理器的本地队列，而逻辑处理器将协程交给内核线程去执行，这样就避免了内核线程的频繁创建与销毁，节约了资源

并且当协程被挂起时，占用内核线程，此时逻辑处理器会与内核线程解绑，去绑定新的内核线程，避免浪费cpu资源，（cpu拿出来给其他内核线程用）

当逻辑处理器的本地队列没有任务时，会向其他逻辑处理器的本地队列进行偷取，或者是从全局队列拿取，这样确保每个逻辑处理器都有工作负载

#### Go垃圾回收

go垃圾回收机制其实就是三色标记发加上混合写屏障
三色标记法就是，将资源对象，根节点标记为黑色，黑色引用的白色对象变为灰色，而下一轮时，灰色变为黑色，黑色引用的白色对象变为灰色，直到不存在灰色对象，此时回收所有白色对象
而在go语言中，为了降低stw时间，标记是和程序并发运行的，也就是说，在标记时可能由于程序运行导致标记过的黑色对象引用了白色对象，最终回收时导致白色对象被回收，会导致异常
而go的处理方式是，将所有黑色对象新引用的对象直接标记被黑色对象，避免白色对象被删除
这种机制叫做写屏障，而go的整体垃圾回收是，首先触发stw，将栈内所有对象标记为黑色，并且堆内开始三色标记加上写屏障，标记完成之后，再次触发stw关闭写屏障并且回收对象

#### Linux线程与Go协程有什么区别

linux的线程是内核线程，创建、阻塞、销毁都需要消耗大量的资源，而go的协程通过GPM模型，让内核线程不被销毁，并且内核线程阻塞时进行挂起，资源消耗少

#### 切片是如何扩容的

小于1024的切片翻倍，大于1024的切片 * 1.25

#### 切片和数组有什么区别

数组是值类型，切片是指向数组的引用

#### Go程序如何与api server进行交互的

使用client-go包，底层使用grpc与api server进行交互

#### GoLang怎么排查堆栈问题

在服务中集成 `pprof`工具

首先判断问题类型，是堆内存泄漏、栈溢出、还是协程泄漏

**如果是栈溢出**，根据panic直接找到对应代码进行调试，可能是边界问题

**排查堆内存泄漏**，在服务刚启动负载正常的时候采集一份基准快照，在服务运行一段时间过后内存明显增长后采集一份快照，并且使用pprof指令对比分析

* 采集基准快照：go tool pprof http://`<host>`:`<port>`/debug/pprof/heap > base.heap
* 快照对比分析：go tool pprof -base base.heap current.heap

并且使用pprof指令排查：

`pprof top` 查看内存增量最大的函数然后通过 `pprof list` 定位到源代码对应的行显示内存分配大小，看看哪个对象内存没有被释放

同时辅助火焰图和Prometheus进行排查

**排查协程泄漏**：和堆泄漏方法类似，首先pprof top找到对应函数然后pprof list列出内存分配，查看goroutine都阻塞在哪里，例如 `channel send`、`channel receive`、`select`，或者channel发送和接收没有配置对

通常堆泄漏是因为全局map和切片，协程泄漏是因为资源未关闭、死锁或阻塞

#### GoLang有哪些只执行一次的函数

sync.Once.Do()

init()

全局变量 + sync/atomic（手动实现）

#### Go如何注意不要内存泄漏

不要让切片频繁扩容

全局对象未被销毁的情况会导致内存泄漏

上下文传递大对象

#### Goroutine如何注意不要内存泄漏

创建链接及时断开

goroutine永久阻塞

### Docker

#### 讲一下DockerFile的多层结构

Docker 镜像采用 UnionFS 的多层只读层结构，每执行一次 Dockerfile 指令都会生成一个新的只读层，层 ID 用内容哈希标识。
构建时每层基于前一层缓存，改动只影响后续层，实现高效增量构建。
容器启动时，在镜像最顶层添加一个可写层，所有运行时修改都写在可写层

#### DockerFile中 COPY 和 ADD 的区别

COPY 仅支持本地文件/目录复制到容器

ADD 除了支持本地文件，还能**自动解压** tar 文件，并且接受 **远程 URL** 下载文件

#### Docker如何打包出比较小的镜像

* **选小基础镜像**
* **多阶段构建，使用上一构建阶段的产物进行构建**
* **合并 RUN 指令** **减少层数；**
* **每层末尾清理缓存**
* **用 .dockerignore** ：排除 .git、缓存、日志等；
* **避免安装不必要工具**

#### Docker镜像如何二次瘦身

可以使用DockerSlim进行瘦身，DockerSlim会通过静态和动态分析，找出真正使用的文件和依赖，生成一个全新的、只包含必要文件的镜像

### Prometheus

#### prometheus是如何采集数据的

使用各种exporter注册到Prometheus的采集规则，Prometheus去对应的exporter组件进行采集

### Kubernetes

#### k8s集群或服务器断网如何排查

这个分为多种情况

1、如果是集群整体链接不到外部网络，查看集群DNS解析是否存在问题：

DNS解析异常可能是DNS服务器问题，如果DNS解析正常，

2、在节点服务器上ping外部服务器和其他节点服务器，如果可以ping不通，是物理服务器问题，使用traceroute看看到哪一步断了，比如到交换机断了可能就是交换机VLAN出现了问题

如果可以ping通，去容器里尝试ping外部服务器和其他节点容器：

如果外部通，其他节点不通，CNI集群内部规则有问题，去查看CNI日志

如果外部不通，其他节点通，说明CNI出口规则有问题，去查看CNI日志

如果外部不通，其他节点不通，说明CNI有问题或者容器iptables有问题，去查看CNI日志

使用traceroute看看到哪步断了

#### 主节点宕机如何排查

先确保集群可用，再排查原因

1、主节点宕机，查看etcd、apiserver、controller manager有没有迁移到其他节点，先确保集群可用

2、如果有alertmanager告警，根据告警详细信息去排查，如果没有告警详细信息

3、查看服务器是否宕机，如果宕机，BMC重启服务器，登录主节点，如果BMC无法重启，硬件问题

4、如果没有宕机，登录主节点，

查看CPU/内存/磁盘 IO是否正常、traceroute是否能联通其他节点、如果无法联通其他节点，看在哪一步断开了，是否因为iptables规则或防火墙原因，或者交换机不通

5、如果磁盘正常、网络正常，查看kubeadm、kubelet、容器运行时是否正常，如果不正常查看日志根据日志调试

6、如果以上都正常但是节点不Ready，查看CNI容器是否正常工作，去容器内traceroute其他节点，根据日志修复CNI

查看kubeadm、kubelet、容器运行时是否正常

#### Kubernetes中探测指针分为几种，如何使用

#### Kubernetes中pod有哪些状态

#### Kubernetes有哪些组件，如何工作

主节点：

apiserver、etcd、controller manger、kube-scheduler

* apiserver：所有请求入口
* etcd：存储集群状态信息
* controller manager：维持容器期望副本数
* kube-scheduler：分配pod到合适节点

工作节点：

kubelet、kube-proxy

* kube-proxy：维护节点上的网络规则
* kubelet：管理 Pod 和容器的生命周期

#### Kubernetes 中 Pod 的创建过程是什么？

* **用户提交** ：kubectl apply 提交 Pod YAML 到  **API Server** 。
* **API 验证** ：API Server 校验权限、格式，存入  **etcd** 。
* **Scheduler 调度** ：Scheduler 监控未调度 Pod，按策略（资源、亲和性）选择  **Node** ，打上 nodeName。
* **Kubelet 监听到** ：目标 Node 的 Kubelet 通过 API Server 发现新 Pod。
* **创建容器** ：Kubelet 调用  **容器运行时** （如 containerd/Docker）拉镜像、启动容器。
* **状态上报** ：Kubelet 持续上报 Pod 状态 → API Server → etcd，用户见 Running。

#### Fannel 与其他 CNI 有什么区别

Fannel：使用**VXLAN** 覆盖网络，不支持网络策略，性能差，兼容性好，基于iptables

Calico：基于BGP协议，三层路由，支持网络策略，性能好

Cilium：基于eBPF，七层路由，支持网络策略，性能最好，

网络策略可以比作pod级别的防火墙，用来实现pod之间的最小权限通信

#### 什么是pause容器？pause容器起到什么作用？

pause容器是所有pod的根容器，pod启动的时候会先启动一个pause容器，创建并持有网络命名空间，分配 Pod 的 IP，维护pod的网络、ip、端口信息。

Pod 生命周期以 pause 容器为准，业务容器挂了 pause 还在，IP 不变；pause 挂了整个 Pod 才挂。

pause 是 Pod 的“网络底座”和“生命周期锚点”。

### Linux

#### iptables配置过吗 X

#### oos Linux常见指令

**1. 文件操作**

* ls -l：列出文件 **详细列表** （权限、用户、时间）
* cd /path：**切换目录**
* pwd：显示**当前路径**
* mkdir：**创建目录**
* rm -rf： **强制递归删除** （危险！）
* cp：**复制**文件/目录
* mv：**移动或重命名**
* touch：**创建空文件或更新时间戳**

**2. 文件查看/编辑**

* cat：**一次性输出文件内容**
* less： **分页查看** （可上下翻）
* head -n 10：显示**前10行**
* tail -f： **实时跟踪日志** （神器）
* vim： **编辑文件** （:w 保存，:q 退出）
* grep "key" file： **搜索关键字** （支持 -r 递归）

**3. 权限管理**

* chmod 755 file：修改权限（ **拥有者可读写执行** ，其他人只读执行）
* chown user:group file：**更改文件所有者**
* sudo：**以管理员身份执行命令**

**4. 系统监控**

* top / htop：**实时查看进程/CPU/内存**
* ps aux：列出**所有进程详情**
* df -h：查看 **磁盘使用** （human-readable）
* du -sh *：统计**当前目录各文件大小**
* free -h：查看**内存使用**
* uptime：显示**开机时长和负载**

**5. 网络 & 进程**

* ping：测试**网络连通性**
* curl： **发送 HTTP 请求** （常用于接口测试）
* wget：**下载文件**
* netstat -tulnp / ss -tulnp：查看**监听端口和进程**
* kill -9： **强制杀进程** （PID）
* systemctl status：查看 **服务状态** （如 nginx、docker）
* nslookup：测试dns解析
* traceroute：测试连通性

### Kubemate

#### 监控模块

**调控**：deploy prometheus operator
**采集**：双statefulset prometheus + 双 sidecar thanos sidecar
**存储优化**：statefulset thanos compactor 通过启动命令行参数配置降采样和保留策略
**查询**：deploy thanos Querier，statefulset thanos Store Gateway

**采集流程**：

1. 每个statefulset prometheus对采集数据进行replcas打标签，并且将固定时长，比如2小时的数据块保留在本地磁盘，
2. thanis sidecar 检测本地磁盘如果发现数据块则上传至minio中，（本地的数据会根据prometheus自己的规则进行保留）

**查询流程**：

1. 通过定制页面访问thanos querier接口，thanos querier去同时访问两个prometheus 获取实时数据，并且访问 thanos gateway获取长期数据
2. 访问 prometheus的部分是实时数据，计算哈希值如果相同直接使用优先级更高的标签来源（将标签靠前的作为主本标签靠后的作为副本进行合并去重），如果不同则都会被展示（比如prometheus A发现了某服务而B没有）
3. 访问thanos store gateway 的部分，thanos store gateway会实现缓存minio 索引，直接拉取数据块

**存储优化**：

1. Thanos compactor通过配置的采样策略，将minio中的数据按照规则降采样和保留
2. 小数据块合并成大数据块，老数据采样降重，原始数据14天，温数据5分钟采样降重3个月，冰数据一小时采样降重1年

**数据暴露**：

1. daemonset node-exporter采集节点服务器数据
2. deploy kube-state-metrics采集k8s集群数据
3. 都通过配置prometheusRules的CRD进行配置采集规则，如果新增或修改规则需要修改对应CR

https://grok.com/share/bGVnYWN5_5698ff1c-609b-45f5-ae3e-9bc2b3776be4
Kubemate页面是如何适配的？监控的粒度如何？都能监控哪些数据

#### 告警模块

**调控**： deploy prometheus-operator
**触发**： statefulset prometheus + CRD PrometheusRule
**处理**： statefulset alertmanager + ConfigMap/Secret alertmanager规则
**通知**： Alertmanager Receivers (集成 Email,钉钉机器人/企业微信机器人)

**告警流程**：

1. **规则评估**： statefulset prometheus 周期性地评估 PrometheusRule CRD 中定义的告警规则。如果满足规则触发告警，告警进入 Pending 状态。
2. **触发告警**： 告警进入 Pending 状态后不会立马推送到 Alertmanager, 而是当告警一段时间内持续满足告警条件 才会触发推送(Firing) ,将告警推送至 Alertmanager ，这样可以避免瞬时数据抖动而触发的虚假告警
3. **接收与去重**： statefulset alertmanager 集群通过 API 接收告警。并对来自多个 prometheus 副本标签一致的告警进行自动去重，确保只处理一次。
4. **处理与路由**： alertmanager 按照 alertmanager.yml 中的配置对告警进行流水线处理：

   • 抑制: 根据告警规则，当一个更高优先级的告警（如集群宕机）触发时，抑制与其相关的低优先级告警（如节点宕机）。
   • 静默: 匹配通过 UI 或 API 动态创建的静默规则，对计划内维护或已知问题产生的告警进行静音处理。
   • 分组: 根据分组 (group_by) 规则（如按alertname, cluster分组），将多个相似告警合并为单个通知。group_wait 定义了初始等待时间以收集更多同组告警。
   • 路由: 根据告警的标签（如severity, team），通过 route 树状结构将告警组匹配到最终的通知接收器（Receiver）。
5. **发送通知**： 路由匹配到的 Receiver 负责将分组后的告警格式化，并通过预设的集成方式（如 Slack, Email, Webhook）发送最终的通知。告警解决后，可配置发送恢复通知。

https://poe.com/s/nHM5snjdOU6R9yaLTlU4
https://poe.com/s/pAStJqpkcNfGbMDSxj6v
Kubemate页面是如何适配的？

#### 日志模块

**采集**：daemonset promtail + deploy loki Distributor + statefulset loki Ingester (以3副本模式运行，负责接收日志、在内存中构建索引和数据块，并最终持久化到对象存储)
**查询**：deploy loki Query Frontend (作为查询API网关，负责缓存、拆分和调度查询) + deploy loki Querier (负责从Ingester和对象存储中获取数据并执行查询)
**存储与优化**：deploy loki Compactor (负责对对象存储中的数据块进行合并和优化) + statefulset MinIO (作为日志块和索引的长期对象存储后端)

**采集流程**：

1. promtail DaemonSet在集群的每一个节点上运行，根据配置发现并追踪目标日志源（如容器标准输出、节点上的日志文件等）。对采集到的日志行进行脱敏、解析，并附加一组标签（Labels），用于索引。
2. promtail将处理后的日志流推送到loki Distributor。若推送失败，日志会暂存于本地的预写日志（ WAL）中，确保数据不丢失。
3. loki Distributor作为负载均衡器，根据哈希环将接收到的日志流分发给多个loki Ingester实例。
4. loki Ingester（配置为3副本）接收日志数据，在内存中构建数据块。当数据块达到一定大小或时间阈值后，将其刷写（Flush）到长期存储MinIO中。为保证数据写入的可靠性，系统配置为至少需要2个副本确认写入成功才算完成。

**查询流程**：

1. 用户通过定制化界面访问loki Query Frontend的API接口。Query Frontend对大型查询进行拆分，并检查缓存中是否有可复用的结果。随后，将查询请求转发给loki Querier。
2. loki Querier并行地向两个数据源发起查询：
   2.1 实时数据：向所有loki Ingester查询尚未刷写到MinIO的近期内存数据。
   2.2 长期数据：向MinIO查询历史日志数据。
3. loki Querier将从Ingester和MinIO获取到的结果进行合并、去重，最终返回给Query Frontend，并呈现给用户。

**存储优化**：
loki Compactor组件会定期扫描对象存储（MinIO）。会将loki Ingester生成的小数据块合并成大数据块，并且同时合并索引，降低查询时扫描文件数量，提升历史日志查询效率

**性能与稳定性保障**：

1. **日志风暴防护**：在promtail的配置中设置了速率限制（每秒1000行）。当某个日志源的流量超过此阈值时，promtail会丢弃超出的日志，并触发告警，防止下游Loki系统被冲垮。同时，系统支持租户级别的流量限制。
2. **高基数问题处理**：
   通过在promtail配置中使用labeldrop等操作，在采集阶段主动丢弃不必要或基数过高的标签。
   通过定时调用Loki API进行分析，主动发现并治理产生高基数标签的日志源。
3. **灾难恢复 (DR)**：
   3.1 **检测**：通过定期的自动化巡检任务，监控Loki集群各组件和底层存储（MinIO）的健康状态。
   3.2 **隔离**：当检测到MinIO中存在损坏的数据块时，具备隔离特定数据块的能力，避免其影响查询服务。
   3.3 **恢复**：
   * **小规模故障**：MinIO采用纠删码（Erasure Coding）模式部署，能够自动恢复单个或少数几个磁盘/节点的故障，保障数据的持久性和可用性。
   * **大规模灾难**：定期将MinIO中的数据进行异地冷备份，用于在发生区域性故障等重大灾难时进行数据恢复。

#### 链路追踪模块

https://poe.com/s/EpMpFS2bTcMnhqUHi83t

#### 服务网格模块

#### CICD模块

目前我们应用平台主要集成了ci模块，主要是tekton对代码仓库的代码进行打包成镜像并推送到镜像仓库，这仅有ci部分，cd部分是手动部署的，我认为完整的cicd应该包括自动部署的内容，

ci过程正常打包镜像到镜像仓库，而argo cd根据git上维护的yaml配置，如果我们需要修改什么组件就上传yaml到git，然后argo cd对比云上的配置和git仓库的配置，如果不同就手动或自动拉取yaml进行部署，这样我们回滚的话直接回滚git即可，也避免了配置漂移，也就是说避免在部署新版本应用之前有人修改了配置文件，而新版本又部署了新的配置，导致配置漂移

#### 为什么选择现有方案，好在哪里？

#### 常规问题

[常规问题](https://hodie-aurora.github.io/2025/06/03/KubemateQA50/)

#### 模块速记

[模块速记](https://hodie-aurora.github.io/2025/06/18/Kubemate-%E6%A8%A1%E5%9D%97%E9%80%9F%E8%AE%B0/)

#### 链路追踪 - Loki 实际问题 - 齐鲁银行 高基数标签问题

[高基数标签问题](https://hodie-aurora.github.io/2025/06/18/Kubemate-Loki-%E9%AB%98%E5%9F%BA%E6%95%B0%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98/)

#### 链路追踪 - Loki 实际问题 - ERP问题 低效LogQL查询问题

[低效LogQL查询问题](https://hodie-aurora.github.io/2025/06/18/Kubemate-Loki-%E4%BD%8E%E6%95%88LogQL%E6%9F%A5%E8%AF%A2%E9%97%AE%E9%A2%98/)

#### 链路追踪 - SkyWalking 实际问题 - 北京昆仑银行 偶发性交易结算数据不一致问题

[偶发性交易结算数据不一致问题](https://hodie-aurora.github.io/2025/06/18/Kubemate-%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA-%E5%81%B6%E5%8F%91%E6%80%A7%E4%BA%A4%E6%98%93%E7%BB%93%E7%AE%97%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98/)

#### 链路追踪 - SkyWalking 实际问题 - 北京光大银行 因安全策略导致的关键业务链路中断 不推荐

[因安全策略导致的关键业务链路中断](https://hodie-aurora.github.io/2025/06/18/Kubemate-%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA-%E5%9B%A0%E5%AE%89%E5%85%A8%E7%AD%96%E7%95%A5%E5%AF%BC%E8%87%B4%E7%9A%84%E5%85%B3%E9%94%AE%E4%B8%9A%E5%8A%A1%E9%93%BE%E8%B7%AF%E4%B8%AD%E6%96%AD/)

#### 链路追踪 - 为什么Jaeger耗能？为什么重启集成Jaeger之后重启时间变为了之前的1.5倍？

#### 链路追踪 - 为什么SkyWalking耗能？

#### 监控模块 - 多集群 Prometheus 怎么确保数据采集不重样

#### 监控模块 - S3存储有了解过吗

#### 实际场景 - 当线上发生错误时如何排查

假如我们部署的业务应用A，发生错误，比如响应码返回500，这会触发Prometheus的监控告警，推动到alertmanager，我们可以根据具体的告警信息来查看是哪个应用的哪个接口发生了错误，并且我们可以在告警信息内获取到对应的traceid，我们可以根据traceid来查询链路追踪这个请求的拓扑图，获取整个访问流程，根据拓扑图我们可以知道在服务内每个接口的http请求参数、载荷、响应时间，请求发生时间，来判断哪一刻哪个方法发生了问题，然后我们可以按照时间可以查询对应应用的日志，找到错误日志查看具体错误，比如说是参数出现问题导致越界，还是内存耗尽触发oom，根据错误进行解决问题，更新存在bug的版本。同时可以借助同时间应用的监控数据来排查，比如查看对应时间段的Prometheus查看是否存在cpu或内存、磁盘异常，或是内存曲线图呈非周期曲线反应内存泄漏等等。

#### 实际场景 - 当线上应用响应缓慢时如何排查

我认为这个排查分为两个方向

首先，应用刚部署的时候，正常流量下是否会出现响应时间慢的问题，如果会，进行访问测试，记录访问请求的traceid，根据traceid去查询链路追踪的拓扑图，找到应用内响应时间最长的span，查看其来源于哪个函数，找到对应函数，查看代码其中有什么耗时操作，是不是查询sql响应缓慢，还是传递的数据包太大，或是逻辑操作不合理，还是远程服务调用出现了问题

如果最开始服务正常，部署时间久了之后发生响应缓慢的问题，我倾向于发生了内存泄漏、连接泄漏、GC压力、，可以在应用最开始部署的时候进行内存快照，然后在部署一段时间响应缓慢的时候再次进行快照，进行对比查看是否有对象没有被回收导致响应缓慢，根据我的经验可能是连接忘记断开导致资源耗尽，或者全局的map或切片一直没有被回收导致不断增大，也科技园查看GC日志进行问题排查

#### 实际场景 - 当最初上线了一个业务之后，业务流量快速扩增，如何判断我们的负载应该扩增多少

我认为这个问题可以从监控指标入手，假如我们目前有10个副本，每台副本cpu使用率为80%，峰值使用率为90%，如果我们预期想将每台副本的cpu使用lv降到60%，峰值使用率降到80%，我们可以计算平时cpu和峰值cpu使用率，来得到扩容后的结果，并且增加冗余机器或者创建HPA，哪怕流量突然增加也可以根据HPA的配置进行扩容

#### 实际场景 - 当云上生产环境部署的应用发布新版本时，需要进行哪些处理

**发布前准备**

在测试环境进行压测处理，并且进行更新和回滚演练

**金丝雀发布**

将小部分流量分流到新的业务，并且确保vip用户的流量在旧版稳定的业务上，等到新版不断迭代稳定之后逐渐增大其流量，如果发生问题则将流量导向旧版本，避免大规模事故

**对照**

对新旧版本的运行数据进行对照分析，观察各项属性如cpu、内存、磁盘、响应时间、响应正确率是否异常，等待新版本平稳运行一段时间后，再关闭旧版本应用实例

#### 最复杂的模块是哪个？（分为开发/部署）

#### 需要改进的模块是哪个？（CICD）

[CICD-集成-GitOps-应用交付中心](https://hodie-aurora.github.io/2025/06/18/Kubemate-CICD-%E9%9B%86%E6%88%90-GitOps-%E5%BA%94%E7%94%A8%E4%BA%A4%E4%BB%98%E4%B8%AD%E5%BF%83/)

### Others

#### 你的技术选型都是怎样决定的？是通过开源社区还是什么？都有哪些方面的考量？

主要是目前主流成熟的技术栈，我认为技术栈不一定要花哨，适合业务并且保留拓展空间的技术栈才是最好的

#### TCP三握四挥

**TCP 三次握手：**

1. **客户端 → 服务端** ：SYN（seq=x）
2. **服务端 → 客户端** ：SYN+ACK（seq=y, ack=x+1）
3. **客户端 → 服务端** ：ACK（seq=x+1, ack=y+1）

**TCP 四次挥手：**

1. **客户端 → 服务端** ：FIN（seq=x）
2. **服务端 → 客户端** ：ACK（ack=x+1）
3. **服务端 → 客户端** ：FIN（seq=y）
4. **客户端 → 服务端** ：ACK（ack=y+1）

#### 对 JVM 了解有多少

jvm的堆内存分为新生代和老年代

新生代分为伊甸园区，s0区，s1区，伊甸园区的对象如果存活放入s0，s0如果存活放入s1，并且年龄加一的同时s1和s0调换，使用复制算法复制活对象，够15岁放入老年代，使用标记整理清除算法清除空间

#### 开源社区做过哪些贡献

#### 云环境巡检

云环境巡检主要是使用K8S的定时任务触发ansible的巡检流程，会采集服务器的 CPU 内存 磁盘 负载 等信息，并且确保节点、容器、镜像仓库、git仓库等资源的可用性，以及查询备份记录、S3状态、防火墙状态等内容，如果存在偏差则根据编排好的等级规则通过Alertmanager发送不通级别告警

#### 分布式存储

我在安硕的时候行方会采购NAS服务器作为分布式存储，我们自用的会使用nfs服务器，在捷誊的话cloudstack上的存储使用的是通过cloudstack csi使用cloudstack的主存储作为分布式存储，主存储通过多个linstor 部分实现分布式存储

#### minio架构了解吗 X

#### Iptables如何查看规则

iptables -L -n
